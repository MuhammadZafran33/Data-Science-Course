<div align="center">

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     HEADER â€” 100% GitHub-Safe (demolab + shields.io + gh-gifs)
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->

<!-- MAIN ANIMATED TITLE -->
<img src="https://readme-typing-svg.demolab.com?font=Fira+Code&weight=800&size=42&duration=3000&pause=800&color=00D4FF&center=true&vCenter=true&width=1000&height=90&lines=%F0%9F%A7%A0+DEEP+LEARNING;Neural+Networks+%7C+ANN+%7C+CNN+%7C+RNN+%7C+LSTM;Powered+by+TensorFlow+%26+Keras+%F0%9F%94%A5" alt="Deep Learning Header"/>

<!-- SUBTITLE -->
<img src="https://readme-typing-svg.demolab.com?font=Fira+Code&weight=400&size=18&duration=4000&pause=1500&color=7DF9FF&center=true&vCenter=true&width=900&height=45&lines=WsCube+Tech+Data+Science+Course+%7C+Muhammad+Zafran;From+Perceptrons+to+Production-Ready+Neural+Networks" alt="Subtitle"/>

<br/>

<!-- ANIMATED DIVIDER (GitHub-hosted â€” always renders) -->
<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%"/>

<br/>

<!-- BADGE ROW 1 â€” CORE TECH -->
<p>
  <img src="https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white"/>
  <img src="https://img.shields.io/badge/Keras-D00000?style=for-the-badge&logo=keras&logoColor=white"/>
  <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white"/>
  <img src="https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white"/>
  <img src="https://img.shields.io/badge/Jupyter-F37626?style=for-the-badge&logo=jupyter&logoColor=white"/>
</p>

<!-- BADGE ROW 2 â€” ARCHITECTURES -->
<p>
  <img src="https://img.shields.io/badge/ANN-Artificial%20Neural%20Net-00D4FF?style=flat-square"/>
  <img src="https://img.shields.io/badge/CNN-Convolutional%20NN-7DF9FF?style=flat-square"/>
  <img src="https://img.shields.io/badge/RNN-Recurrent%20NN-00BFFF?style=flat-square"/>
  <img src="https://img.shields.io/badge/LSTM-Long%20Short%20Term%20Memory-1E90FF?style=flat-square"/>
  <img src="https://img.shields.io/badge/OpenCV-Computer%20Vision-5C3EE8?style=flat-square&logo=opencv"/>
</p>

<!-- BADGE ROW 3 â€” REPO META -->
<p>
  <img src="https://img.shields.io/badge/Module-Deep%20Learning-00D4FF?style=flat-square"/>
  <img src="https://img.shields.io/badge/Course-WsCube%20Tech-FF6F00?style=flat-square&logo=youtube&logoColor=white"/>
  <img src="https://img.shields.io/badge/Level-Advanced-red?style=flat-square"/>
  <img src="https://img.shields.io/badge/Notebooks-10%2B-success?style=flat-square"/>
  <img src="https://img.shields.io/badge/Status-Active-brightgreen?style=flat-square"/>
  <img src="https://img.shields.io/github/last-commit/MuhammadZafran33/Data-Science-Course?style=flat-square&color=00D4FF"/>
</p>

<br/>

> ### ğŸ§  *"Deep Learning is a superpower. With it, you can make a computer see, synthesize novel art, translate languages, render a medical diagnosis, or build pieces of a car that can drive itself."*
> **â€” Andrew Ng**

<br/>

</div>

---

## ğŸ“š Table of Contents

| # | Section | Quick Link |
|---|---------|-----------|
| 01 | ğŸ—ºï¸ Module Overview | [Jump](#ï¸-module-overview) |
| 02 | ğŸ§­ Learning Roadmap | [Jump](#-learning-roadmap) |
| 03 | ğŸ§© Architecture Deep Dive | [Jump](#-architecture-deep-dive) |
| 04 | ğŸ“Š Coverage Charts | [Jump](#-coverage-charts) |
| 05 | âš¡ Activation Functions | [Jump](#-activation-functions-cheatsheet) |
| 06 | ğŸ›ï¸ Optimizers Guide | [Jump](#ï¸-optimizers-guide) |
| 07 | ğŸ—ï¸ Which Architecture? | [Jump](#ï¸-which-architecture-to-use) |
| 08 | ğŸ“ Key Concepts & Glossary | [Jump](#-key-concepts--glossary) |
| 09 | ğŸ”¬ Layer Types Reference | [Jump](#-layer-types-reference) |
| 10 | ğŸ“ Folder Structure | [Jump](#-folder-structure) |
| 11 | ğŸ› ï¸ Tools & Libraries | [Jump](#ï¸-tools--libraries) |
| 12 | ğŸ’» Quick Code Reference | [Jump](#-quick-code-reference) |
| 13 | ğŸš€ Getting Started | [Jump](#-getting-started) |
| 14 | ğŸ”— Navigation | [Jump](#-navigation) |

---

## ğŸ—ºï¸ Module Overview

<div align="center">

| ğŸ“Œ Attribute | ğŸ“‹ Details |
|-------------|-----------|
| ğŸ“ **Parent Course** | Data Science Full Course â€” WsCube Tech |
| ğŸ“‚ **Module Name** | Deep Learning |
| ğŸ“ **Position in Course** | Advanced Module (after Machine Learning) |
| â±ï¸ **Study Duration** | 3â€“4 Weeks Â· 25+ Hours |
| ğŸ““ **Notebooks** | 10+ Jupyter Notebooks |
| ğŸ§  **Core Architectures** | ANN Â· CNN Â· RNN Â· LSTM |
| ğŸ”§ **Primary Framework** | TensorFlow 2.x + Keras API |
| ğŸ¯ **Why It Matters** | Powers image recognition, NLP, forecasting, self-driving cars, and generative AI |

</div>

---

## ğŸ§­ Learning Roadmap

```mermaid
flowchart TD
    START(["ğŸš€ START\nDeep Learning Module"])

    START --> A["ğŸ§© Foundations\nWhat is Deep Learning?\nML vs DL Â· Applications\nBiological Neuron Analogy"]

    A --> B["âš¡ The Building Block\nPerceptron & MLP\nForward Propagation\nActivation Functions"]

    B --> C["ğŸ”„ How Networks Learn\nBackpropagation\nGradient Descent\nLoss Functions Â· Optimizers"]

    C --> D["ğŸ§ª Training Best Practices\nOverfitting & Underfitting\nDropout Â· Batch Norm\nEarly Stopping Â· Regularization"]

    D --> E1["ğŸ¤– ANN\nArtificial Neural Networks\nTabular Data\nClassification & Regression"]
    D --> E2["ğŸ–¼ï¸ CNN\nConvolutional Neural Nets\nImage Classification\nFeature Maps Â· Pooling"]
    D --> E3["ğŸ” RNN & LSTM\nSequential Data\nTime Series\nText Generation"]

    E1 --> F["ğŸ‘ï¸ Computer Vision\nOpenCV Basics\nImage Preprocessing\nObject Detection"]

    E2 --> F
    E3 --> G["ğŸ“Š Projects\nEnd-to-End DL Apps\nKaggle Competitions\nModel Deployment"]
    F --> G

    G --> END(["âœ… COMPLETE\nDeep Learning Expert"])

    style START fill:#00D4FF,stroke:none,color:#000
    style END  fill:#00ff88,stroke:none,color:#000
    style A    fill:#0a1628,stroke:#00D4FF,color:#7DF9FF
    style B    fill:#0a1628,stroke:#00D4FF,color:#7DF9FF
    style C    fill:#0a1628,stroke:#7DF9FF,color:#7DF9FF
    style D    fill:#0a1628,stroke:#7DF9FF,color:#7DF9FF
    style E1   fill:#1a0533,stroke:#00D4FF,color:#c4b5fd
    style E2   fill:#1a0533,stroke:#00D4FF,color:#c4b5fd
    style E3   fill:#1a0533,stroke:#00D4FF,color:#c4b5fd
    style F    fill:#0d1b2a,stroke:#7DF9FF,color:#7DF9FF
    style G    fill:#003366,stroke:#00D4FF,color:#7DF9FF
```

---

## ğŸ§© Architecture Deep Dive

### ğŸ¤– Part 1 â€” Artificial Neural Network (ANN)

> Best for: **Tabular / Structured Data** â€” classification & regression problems

```mermaid
flowchart LR
    subgraph INPUT ["ğŸ“¥ Input Layer"]
        I1(["xâ‚"])
        I2(["xâ‚‚"])
        I3(["xâ‚ƒ"])
        I4(["xâ‚„"])
    end

    subgraph H1 ["ğŸ”µ Hidden Layer 1"]
        H11(["nâ‚"])
        H12(["nâ‚‚"])
        H13(["nâ‚ƒ"])
        H14(["nâ‚„"])
    end

    subgraph H2 ["ğŸŸ£ Hidden Layer 2"]
        H21(["nâ‚"])
        H22(["nâ‚‚"])
        H23(["nâ‚ƒ"])
    end

    subgraph OUTPUT ["ğŸ“¤ Output Layer"]
        O1(["Å·"])
    end

    I1 & I2 & I3 & I4 --> H11 & H12 & H13 & H14
    H11 & H12 & H13 & H14 --> H21 & H22 & H23
    H21 & H22 & H23 --> O1

    style INPUT fill:#0a1628,stroke:#00D4FF,color:#7DF9FF
    style H1    fill:#1a0533,stroke:#c4b5fd,color:#c4b5fd
    style H2    fill:#003366,stroke:#7DF9FF,color:#7DF9FF
    style OUTPUT fill:#00D4FF,stroke:none,color:#000
```

| ğŸ”‘ Concept | ğŸ“‹ Description | Notebook |
|-----------|---------------|:--------:|
| **Perceptron** | Single neuron â€” weighted sum + activation | `01_perceptron.ipynb` |
| **MLP** | Multi-Layer Perceptron â€” stacked dense layers | `02_ann_basics.ipynb` |
| **Forward Pass** | Input â†’ Hidden â†’ Output (predictions) | `02_ann_basics.ipynb` |
| **Backpropagation** | Compute gradients, update weights via chain rule | `03_backprop.ipynb` |
| **Loss Functions** | MSE (regression) Â· Binary/Categorical CE (classification) | `03_backprop.ipynb` |
| **Optimizers** | SGD Â· Adam Â· RMSprop â€” update weights to minimize loss | `04_optimizers.ipynb` |
| **ANN Project** | Customer churn prediction Â· House price prediction | `05_ann_project.ipynb` |

---

### ğŸ–¼ï¸ Part 2 â€” Convolutional Neural Network (CNN)

> Best for: **Images & Visual Data** â€” classification, detection, segmentation

```mermaid
flowchart LR
    IMG(["ğŸ–¼ï¸ Input Image\n32Ã—32Ã—3"]) -->|"Conv2D\n+ReLU"| C1["Feature Maps\n30Ã—30Ã—32"]
    C1 -->|"MaxPooling\n2Ã—2"| P1["15Ã—15Ã—32"]
    P1 -->|"Conv2D\n+ReLU"| C2["13Ã—13Ã—64"]
    C2 -->|"MaxPooling\n2Ã—2"| P2["6Ã—6Ã—64"]
    P2 -->|"Flatten"| FL["2304 units"]
    FL -->|"Dense\n+ReLU"| D1["128 units"]
    D1 -->|"Dropout\n0.5"| DR["128 units"]
    DR -->|"Dense\n+Softmax"| OUT(["ğŸ“¤ Predictions\n10 classes"])

    style IMG fill:#0a1628,stroke:#00D4FF,color:#7DF9FF
    style C1  fill:#1a0533,stroke:#c4b5fd,color:#c4b5fd
    style P1  fill:#003366,stroke:#7DF9FF,color:#7DF9FF
    style C2  fill:#1a0533,stroke:#c4b5fd,color:#c4b5fd
    style P2  fill:#003366,stroke:#7DF9FF,color:#7DF9FF
    style FL  fill:#0a1628,stroke:#00D4FF,color:#7DF9FF
    style D1  fill:#1a0533,stroke:#c4b5fd,color:#c4b5fd
    style DR  fill:#003366,stroke:#7DF9FF,color:#7DF9FF
    style OUT fill:#00D4FF,stroke:none,color:#000
```

| ğŸ”‘ Concept | ğŸ“‹ Description | Notebook |
|-----------|---------------|:--------:|
| **Convolution** | Filter slides over image to extract spatial features | `06_cnn_basics.ipynb` |
| **Feature Maps** | Output of applying a convolutional filter | `06_cnn_basics.ipynb` |
| **Pooling** | MaxPool/AvgPool â€” reduce spatial dimensions | `06_cnn_basics.ipynb` |
| **Padding** | `same` or `valid` â€” controls output size | `06_cnn_basics.ipynb` |
| **Stride** | Step size of the sliding filter | `06_cnn_basics.ipynb` |
| **Flatten + Dense** | Convert feature maps to 1D â†’ classification head | `07_cnn_project.ipynb` |
| **Data Augmentation** | Flip, rotate, zoom â€” fight overfitting | `07_cnn_project.ipynb` |
| **Transfer Learning** | VGG16, ResNet, MobileNet â€” use pretrained weights | `08_transfer_learning.ipynb` |
| **CNN Project** | CIFAR-10 Â· MNIST Digit Classifier Â· Cats vs Dogs | `07_cnn_project.ipynb` |

---

### ğŸ” Part 3 â€” Recurrent Neural Network (RNN) & LSTM

> Best for: **Sequential / Time-Series Data** â€” NLP, forecasting, stock prices

```mermaid
flowchart LR
    subgraph RNN_CELL ["ğŸ” RNN Cell (unrolled)"]
        direction LR
        T1["xâ‚"] -->|"Wâ‚“"| H1_RNN["hâ‚œâ‚‹â‚‚"]
        H1_RNN -->|"Wâ‚•"| H2["hâ‚œâ‚‹â‚"]
        T2["xâ‚‚"] --> H2
        H2 -->|"Wâ‚•"| H3["hâ‚œ"]
        T3["xâ‚ƒ"] --> H3
        H3 --> O_RNN["Å·"]
    end

    subgraph LSTM_CELL ["ğŸ§  LSTM Cell â€” 4 Gates"]
        direction TB
        FG["ğŸšª Forget Gate\nWhat to forget"] --> CS["ğŸ“¦ Cell State\nLong-term memory"]
        IG["â• Input Gate\nWhat to add"] --> CS
        CS --> OG["ğŸ“¤ Output Gate\nWhat to output"]
        OG --> H_LSTM["hâ‚œ Hidden State"]
    end

    RNN_CELL --> LSTM_CELL

    style RNN_CELL fill:#0a1628,stroke:#00D4FF,color:#7DF9FF
    style LSTM_CELL fill:#1a0533,stroke:#c4b5fd,color:#c4b5fd
    style FG fill:#003366,stroke:#7DF9FF,color:#7DF9FF
    style IG fill:#003366,stroke:#7DF9FF,color:#7DF9FF
    style CS fill:#00D4FF,stroke:none,color:#000
    style OG fill:#003366,stroke:#7DF9FF,color:#7DF9FF
    style H_LSTM fill:#1a0533,stroke:#c4b5fd,color:#c4b5fd
```

| ğŸ”‘ Concept | ğŸ“‹ Description | Notebook |
|-----------|---------------|:--------:|
| **Vanilla RNN** | Processes sequences step-by-step with hidden state | `09_rnn_basics.ipynb` |
| **Vanishing Gradient** | RNNs struggle with long sequences â€” gradients shrink | `09_rnn_basics.ipynb` |
| **LSTM** | Long Short-Term Memory â€” solves vanishing gradient | `10_lstm.ipynb` |
| **Forget Gate** | Decides what info to discard from cell state | `10_lstm.ipynb` |
| **Input Gate** | Decides what new info to store in cell state | `10_lstm.ipynb` |
| **Output Gate** | Decides what part of cell state to output | `10_lstm.ipynb` |
| **GRU** | Gated Recurrent Unit â€” simplified, faster than LSTM | `10_lstm.ipynb` |
| **RNN/LSTM Project** | Stock price prediction Â· Sentiment analysis Â· Text gen | `11_rnn_project.ipynb` |

---

### ğŸ‘ï¸ Part 4 â€” Computer Vision with OpenCV

| ğŸ”‘ Concept | ğŸ“‹ Description | Notebook |
|-----------|---------------|:--------:|
| **Image Reading** | `cv2.imread()` Â· Channels (BGR vs RGB) | `12_opencv_basics.ipynb` |
| **Image Preprocessing** | Resize Â· Normalize Â· Grayscale conversion | `12_opencv_basics.ipynb` |
| **Image Operations** | Blur Â· Edge detection Â· Thresholding | `12_opencv_basics.ipynb` |
| **Object Detection** | Haar Cascades Â· YOLO integration concepts | `12_opencv_basics.ipynb` |
| **Video Processing** | Reading frames from webcam / video files | `12_opencv_basics.ipynb` |

---

## ğŸ“Š Coverage Charts

### Topic Distribution

```mermaid
pie title Deep Learning Module â€” Content Distribution
    "ANN Foundations and Training" : 25
    "CNN and Computer Vision" : 28
    "RNN and LSTM" : 22
    "OpenCV and Image Processing" : 10
    "Regularization and Optimization" : 10
    "Projects and Deployment" : 5
```

### Study Hours Per Topic

```mermaid
xychart-beta
    title "Estimated Study Hours by Topic"
    x-axis ["DL Intro", "Perceptron MLP", "Backprop", "Optimizers", "Regularize", "ANN Project", "CNN Basics", "CNN Project", "Transfer Learn", "RNN LSTM", "RNN Project", "OpenCV"]
    y-axis "Hours" 0 --> 5
    bar [1.5, 2, 2.5, 2, 2, 3, 2.5, 3, 2.5, 3, 3, 2]
```

### Module Timeline in Full Course

```mermaid
gantt
    title Deep Learning Position in WsCube Tech DS Course
    dateFormat  X
    axisFormat  Week %s

    section Foundations
    Python and Stats         :done,    f1, 1, 5
    NumPy and Pandas         :done,    f2, 5, 9

    section Visualization
    Matplotlib and EDA       :done,    v1, 9, 13

    section Machine Learning
    Supervised ML            :done,    ml1, 13, 18
    Unsupervised ML          :done,    ml2, 18, 21

    section Deep Learning
    ANN Foundations          :active,  dl1, 21, 23
    CNN and Computer Vision  :active,  dl2, 23, 25
    RNN and LSTM             :         dl3, 25, 27

    section Capstone
    Projects and Deployment  :         cp1, 27, 29
```

---

## âš¡ Activation Functions Cheatsheet

```mermaid
flowchart LR
    subgraph SIGMOID ["ğŸ”µ Sigmoid  Ïƒ(x) = 1/(1+eâ»Ë£)"]
        S1["Range: 0 â†’ 1\nUse: Binary output\nâš ï¸ Vanishing gradient"]
    end

    subgraph TANH ["ğŸŸ£ Tanh  tanh(x)"]
        T1["Range: -1 â†’ 1\nUse: Hidden layers\nâš ï¸ Still vanishing"]
    end

    subgraph RELU ["ğŸŸ¢ ReLU  max(0, x)"]
        R1["Range: 0 â†’ âˆ\nUse: Hidden layers âœ…\nâš ï¸ Dying ReLU"]
    end

    subgraph LEAKY ["ğŸŸ¡ Leaky ReLU"]
        L1["Range: -âˆ â†’ âˆ\nFixes dying ReLU\nUse: Hidden layers"]
    end

    subgraph SOFTMAX ["ğŸ”´ Softmax"]
        SM1["Range: 0 â†’ 1 (sums=1)\nUse: Multi-class output âœ…\nConverts to probabilities"]
    end

    style SIGMOID fill:#1a0533,stroke:#c4b5fd,color:#e9d5ff
    style TANH    fill:#003366,stroke:#7DF9FF,color:#7DF9FF
    style RELU    fill:#0a1628,stroke:#00D4FF,color:#7DF9FF
    style LEAKY   fill:#1a0533,stroke:#c4b5fd,color:#e9d5ff
    style SOFTMAX fill:#003366,stroke:#7DF9FF,color:#7DF9FF
```

| Function | Formula | Range | Best Used For | Layer Type |
|----------|---------|-------|--------------|------------|
| **Sigmoid** | `1 / (1 + eâ»Ë£)` | (0, 1) | Binary classification output | Output |
| **Tanh** | `(eË£ âˆ’ eâ»Ë£)/(eË£ + eâ»Ë£)` | (-1, 1) | RNN hidden states | Hidden |
| **ReLU** | `max(0, x)` | [0, âˆ) | Most hidden layers âœ… Most popular | Hidden |
| **Leaky ReLU** | `max(0.01x, x)` | (-âˆ, âˆ) | When dying ReLU is an issue | Hidden |
| **ELU** | `x if x>0 else Î±(eË£âˆ’1)` | (-Î±, âˆ) | Smoother than ReLU | Hidden |
| **Softmax** | `eË£â± / Î£eË£Ê²` | (0, 1) | Multi-class output âœ… | Output |

---

## ğŸ›ï¸ Optimizers Guide

```mermaid
flowchart TD
    Q(["Which Optimizer?"]) --> A{"Know learning rate?"}

    A -- "Yes, tuning manually" --> B["âš¡ SGD\nSimple Â· Fast Â· Noisy\nNeeds LR schedule"]
    A -- "No, use adaptive" --> C{"Dataset type?"}

    C -- "General / Most cases" --> D["ğŸ† Adam\nAdaptive Â· Fast Converge\nBest default choice âœ…"]
    C -- "RNN or NLP tasks" --> E["ğŸ“ˆ RMSprop\nAdaptive LR Â· Good for RNNs"]
    C -- "Very noisy gradients" --> F["ğŸ”§ AdaGrad\nAdaptive per param\nDecays over time"]

    style Q  fill:#00D4FF,stroke:none,color:#000
    style D  fill:#00ff88,stroke:none,color:#000
    style B  fill:#0a1628,stroke:#00D4FF,color:#7DF9FF
    style C  fill:#1a0533,stroke:#c4b5fd,color:#e9d5ff
    style E  fill:#003366,stroke:#7DF9FF,color:#7DF9FF
    style F  fill:#0a1628,stroke:#00D4FF,color:#7DF9FF
```

| Optimizer | Best For | Key Params | Keras Usage |
|-----------|---------|-----------|-------------|
| **SGD** | Simple tasks, custom LR schedules | `lr`, `momentum` | `optimizer='sgd'` |
| **Adam** â­ | Most tasks â€” best default | `lr=0.001`, `beta_1`, `beta_2` | `optimizer='adam'` |
| **RMSprop** | RNNs and sequential models | `lr`, `rho` | `optimizer='rmsprop'` |
| **AdaGrad** | Sparse data / NLP | `lr` | `optimizer='adagrad'` |
| **AdamW** | Regularized training | `lr`, `weight_decay` | `tf.keras.optimizers.AdamW` |

---

## ğŸ—ï¸ Which Architecture to Use?

```mermaid
flowchart TD
    START(["ğŸ¤” What is your data type?"]) --> A{"Data Type?"}

    A -- "ğŸ“Š Tabular / CSV / Structured" --> ANN_BOX["ğŸ¤– Use ANN\nCustomer churn, pricing\nfraud detection, regression"]

    A -- "ğŸ–¼ï¸ Images / Video / Spatial" --> CNN_BOX["ğŸ–¼ï¸ Use CNN\nImage classification\nobject detection, OCR"]

    A -- "ğŸ“ Text / Sequences / Time-series" --> B{"Problem type?"}

    B -- "Short sequences" --> RNN_BOX["ğŸ” Use RNN\nShort text, basic\nsequence tagging"]
    B -- "Long sequences / Memory needed" --> LSTM_BOX["ğŸ§  Use LSTM or GRU\nTime-series, sentiment\ntext generation, translation"]

    A -- "Both Image + Sequence" --> MIX["ğŸ”€ Hybrid Model\nCNN + LSTM\nVideo captioning, action recognition"]

    style START   fill:#00D4FF,stroke:none,color:#000
    style ANN_BOX fill:#003366,stroke:#7DF9FF,color:#7DF9FF
    style CNN_BOX fill:#1a0533,stroke:#c4b5fd,color:#e9d5ff
    style RNN_BOX fill:#0a1628,stroke:#00D4FF,color:#7DF9FF
    style LSTM_BOX fill:#003366,stroke:#7DF9FF,color:#7DF9FF
    style MIX      fill:#1a0533,stroke:#c4b5fd,color:#e9d5ff
    style B        fill:#0a1628,stroke:#00D4FF,color:#7DF9FF
```

### Architecture Comparison Table

| Feature | ğŸ¤– ANN | ğŸ–¼ï¸ CNN | ğŸ” RNN | ğŸ§  LSTM |
|---------|:------:|:------:|:------:|:-------:|
| **Data Type** | Tabular | Images | Sequences | Long Sequences |
| **Spatial Awareness** | âŒ | âœ… | âŒ | âŒ |
| **Sequential Memory** | âŒ | âŒ | âœ… (short) | âœ… (long) |
| **Translation Invariance** | âŒ | âœ… | âŒ | âŒ |
| **Vanishing Gradient** | Moderate | Low | High âš ï¸ | Solved âœ… |
| **Computational Cost** | Low | High | Medium | High |
| **Typical Use** | Churn, Pricing | Vision, OCR | Basic NLP | Time-series, NLP |
| **Keras Layer** | `Dense` | `Conv2D` | `SimpleRNN` | `LSTM` |

---

## ğŸ“ Key Concepts & Glossary

<div align="center">

| ğŸ“Œ Term | ğŸ’¡ Definition |
|--------|--------------|
| **Epoch** | One full pass of the training data through the model |
| **Batch Size** | Number of samples processed before updating weights |
| **Learning Rate** | Step size for weight updates â€” too high=diverge, too low=slow |
| **Overfitting** | Model memorizes training data, fails on new data |
| **Underfitting** | Model is too simple to capture patterns |
| **Dropout** | Randomly zeros neurons during training â€” prevents overfitting |
| **Batch Normalization** | Normalizes layer inputs â€” stabilizes & speeds up training |
| **Weight Initialization** | How weights are set at start â€” He, Xavier, Random |
| **Gradient Descent** | Optimization algorithm â€” move weights in direction of steepest descent |
| **Vanishing Gradient** | Gradients become tiny in deep networks â€” weights stop updating |
| **Transfer Learning** | Use pretrained model weights as starting point |
| **Fine-Tuning** | Unfreeze pretrained layers and train on new data |
| **Confusion Matrix** | True Positive/Negative, False Positive/Negative â€” classification evaluation |
| **ROC-AUC** | Area Under Curve â€” how well model separates classes |

</div>

---

## ğŸ”¬ Layer Types Reference

| Layer | Keras API | Purpose | Used In |
|-------|-----------|---------|---------|
| **Dense** | `layers.Dense(units, activation)` | Fully connected â€” any problem | ANN, CNN head |
| **Conv2D** | `layers.Conv2D(filters, kernel_size)` | 2D spatial convolution | CNN |
| **MaxPooling2D** | `layers.MaxPooling2D(pool_size)` | Spatial downsampling | CNN |
| **Flatten** | `layers.Flatten()` | Convert 2D feature maps to 1D | CNN â†’ Dense |
| **SimpleRNN** | `layers.SimpleRNN(units)` | Vanilla recurrent layer | Sequences |
| **LSTM** | `layers.LSTM(units, return_sequences)` | Long-short term memory | NLP, time-series |
| **GRU** | `layers.GRU(units)` | Gated recurrent, lighter than LSTM | NLP, time-series |
| **Embedding** | `layers.Embedding(vocab_size, dim)` | Text â†’ dense vectors | NLP |
| **Dropout** | `layers.Dropout(rate)` | Regularization â€” zero out neurons | All models |
| **BatchNorm** | `layers.BatchNormalization()` | Normalize layer activations | All models |
| **GlobalAvgPool2D** | `layers.GlobalAveragePooling2D()` | Aggregate spatial features | CNN head |

---

## ğŸ“ Folder Structure

```
ğŸ“‚ DEEP LEARNING/
â”‚
â”œâ”€â”€ ğŸ““ 01_intro_to_deep_learning.ipynb
â”‚   â””â”€â”€ â†’ ML vs DL Â· Biological neurons Â· Applications Â· Frameworks
â”‚
â”œâ”€â”€ ğŸ““ 02_ann_perceptron_mlp.ipynb
â”‚   â””â”€â”€ â†’ Perceptron Â· MLP Â· Forward pass Â· Architecture
â”‚
â”œâ”€â”€ ğŸ““ 03_backpropagation_loss.ipynb
â”‚   â””â”€â”€ â†’ Chain rule Â· Gradient descent Â· MSE Â· Cross-entropy
â”‚
â”œâ”€â”€ ğŸ““ 04_activation_functions.ipynb
â”‚   â””â”€â”€ â†’ ReLU Â· Sigmoid Â· Tanh Â· Softmax Â· Leaky ReLU
â”‚
â”œâ”€â”€ ğŸ““ 05_optimizers_training.ipynb
â”‚   â””â”€â”€ â†’ SGD Â· Adam Â· RMSprop Â· LR schedules Â· Early stopping
â”‚
â”œâ”€â”€ ğŸ““ 06_regularization.ipynb
â”‚   â””â”€â”€ â†’ Dropout Â· Batch normalization Â· L1/L2 Â· Overfitting fix
â”‚
â”œâ”€â”€ ğŸ““ 07_ann_project_churn.ipynb
â”‚   â””â”€â”€ â†’ Telecom churn prediction end-to-end
â”‚
â”œâ”€â”€ ğŸ““ 08_cnn_basics.ipynb
â”‚   â””â”€â”€ â†’ Conv2D Â· MaxPooling Â· Feature maps Â· Padding Â· Stride
â”‚
â”œâ”€â”€ ğŸ““ 09_cnn_project_image_classification.ipynb
â”‚   â””â”€â”€ â†’ MNIST / CIFAR-10 Â· Data augmentation Â· Training pipeline
â”‚
â”œâ”€â”€ ğŸ““ 10_transfer_learning.ipynb
â”‚   â””â”€â”€ â†’ VGG16 Â· ResNet50 Â· MobileNet Â· Fine-tuning
â”‚
â”œâ”€â”€ ğŸ““ 11_rnn_basics.ipynb
â”‚   â””â”€â”€ â†’ Vanilla RNN Â· Vanishing gradient Â· Sequence modeling
â”‚
â”œâ”€â”€ ğŸ““ 12_lstm_gru.ipynb
â”‚   â””â”€â”€ â†’ LSTM gates Â· GRU Â· Return sequences Â· Stacked LSTM
â”‚
â”œâ”€â”€ ğŸ““ 13_rnn_project_stock_prediction.ipynb
â”‚   â””â”€â”€ â†’ Time-series forecasting with LSTM
â”‚
â”œâ”€â”€ ğŸ““ 14_opencv_basics.ipynb
â”‚   â””â”€â”€ â†’ cv2.imread Â· Resize Â· Grayscale Â· Edge detection
â”‚
â””â”€â”€ ğŸ“„ README.md
```

---

## ğŸ› ï¸ Tools & Libraries

<div align="center">

| ğŸ“¦ Library | ğŸ¯ Role | ğŸ’¡ Key APIs |
|-----------|--------|------------|
| ![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=flat-square&logo=tensorflow&logoColor=white) | Core DL framework by Google | `tf.keras`, `tf.data`, `tf.GradientTape` |
| ![Keras](https://img.shields.io/badge/Keras-D00000?style=flat-square&logo=keras&logoColor=white) | High-level NN API (inside TF) | `Sequential`, `Model`, `layers.*` |
| ![NumPy](https://img.shields.io/badge/NumPy-013243?style=flat-square&logo=numpy&logoColor=white) | Array ops & data prep | `np.array`, `reshape`, `expand_dims` |
| ![Pandas](https://img.shields.io/badge/Pandas-150458?style=flat-square&logo=pandas&logoColor=white) | Load & preprocess datasets | `read_csv`, `get_dummies`, `fillna` |
| ![Matplotlib](https://img.shields.io/badge/Matplotlib-11557C?style=flat-square) | Training curve plots | `plt.plot(history.history['loss'])` |
| ![OpenCV](https://img.shields.io/badge/OpenCV-5C3EE8?style=flat-square&logo=opencv&logoColor=white) | Computer vision ops | `cv2.imread`, `cv2.resize`, `cv2.cvtColor` |
| ![Scikit-learn](https://img.shields.io/badge/Scikit--learn-F7931E?style=flat-square&logo=scikit-learn&logoColor=white) | Preprocessing & evaluation | `StandardScaler`, `train_test_split`, `classification_report` |

</div>

---

## ğŸ’» Quick Code Reference

### âš¡ Build & Train an ANN (Keras)

```python
import tensorflow as tf
from tensorflow.keras import layers, Sequential

# Build model
model = Sequential([
    layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    layers.BatchNormalization(),
    layers.Dropout(0.3),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(1, activation='sigmoid')   # Binary classification
])

# Compile
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train with early stopping
early_stop = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)
history = model.fit(X_train, y_train,
                    epochs=100,
                    batch_size=32,
                    validation_split=0.2,
                    callbacks=[early_stop])
```

### ğŸ–¼ï¸ Build a CNN for Image Classification

```python
model = Sequential([
    layers.Conv2D(32, (3,3), activation='relu', input_shape=(64, 64, 3)),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(128, (3,3), activation='relu'),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')  # 10 classes
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
```

### ğŸ” Build an LSTM for Time-Series

```python
model = Sequential([
    layers.LSTM(64, return_sequences=True, input_shape=(timesteps, features)),
    layers.Dropout(0.2),
    layers.LSTM(32, return_sequences=False),
    layers.Dropout(0.2),
    layers.Dense(1)   # Regression output (next value)
])

model.compile(optimizer='adam', loss='mse')
```

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone & Navigate

```bash
git clone https://github.com/MuhammadZafran33/Data-Science-Course.git
cd "Data-Science-Course/Data Science Full Course By WsCube Tech/DEEP LEARNING"
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install tensorflow keras numpy pandas matplotlib seaborn opencv-python scikit-learn jupyter
```

### 3ï¸âƒ£ Launch Notebooks

```bash
jupyter notebook
```

> â˜ï¸ **No GPU locally? Run free on Google Colab with GPU runtime:**

<div align="center">

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MuhammadZafran33/Data-Science-Course/)

</div>

### âœ… Recommended Study Order

```mermaid
journey
    title Deep Learning Study Journey
    section Week 1 â€” ANN Core
      DL Intro and Neurons: 9: Learner
      Perceptron and MLP: 9: Learner
      Backpropagation: 8: Learner
      Activation Functions: 8: Learner
      Optimizers and Training: 8: Learner
    section Week 2 â€” CNN and Vision
      Regularization: 7: Learner
      ANN Project Churn: 8: Learner
      CNN Basics: 8: Learner
      CNN Project CIFAR-10: 7: Learner
      Transfer Learning: 6: Learner
    section Week 3 â€” RNN and Sequences
      RNN Basics: 7: Learner
      LSTM and GRU: 7: Learner
      RNN Project Stocks: 6: Learner
      OpenCV Basics: 6: Learner
```

---

## ğŸ”— Navigation

<div align="center">

| â¬…ï¸ Previous Module | ğŸ“ You Are Here | â¡ï¸ Next Module |
|-------------------|----------------|---------------|
| [ğŸ¤– Machine Learning](../MACHINE%20LEARNING/) | **ğŸ§  Deep Learning** | [ğŸ’¬ NLP / Text Mining â†’](../NLP/) |

</div>

---

<div align="center">

<br/>

[![GitHub](https://img.shields.io/badge/GitHub-MuhammadZafran33-181717?style=for-the-badge&logo=github&logoColor=white)](https://github.com/MuhammadZafran33)

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%"/>

> *"The brain is the last and grandest biological frontier, the most complex thing we have yet discovered in our universe."*
> **â€” James D. Watson**

<br/>

**â­ If this helped your Deep Learning journey â€” drop a star! â­**

<br/>

<!-- FOOTER TYPING ANIMATION -->
<img src="https://readme-typing-svg.demolab.com?font=Fira+Code&size=14&pause=1000&color=00D4FF&center=true&vCenter=true&width=700&lines=Keep+Building+Neural+Networks+%F0%9F%A7%A0;Every+Forward+Pass+Gets+You+Closer+to+Mastery+%F0%9F%9A%80" alt="Footer"/>

</div>
