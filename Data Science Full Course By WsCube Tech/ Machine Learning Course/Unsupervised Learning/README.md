# ğŸ¤– Unsupervised Learning - Complete Mastery Guide

<div align="center">

![Unsupervised Learning](https://img.shields.io/badge/Machine%20Learning-Unsupervised%20Learning-blue?style=for-the-badge&logo=python)
![Status](https://img.shields.io/badge/Status-Complete-brightgreen?style=for-the-badge)
![Difficulty](https://img.shields.io/badge/Difficulty-Intermediate%20to%20Advanced-orange?style=for-the-badge)

**Master the Art of Learning Without Labels** ğŸ¯

A comprehensive, hands-on course on Unsupervised Learning algorithms, techniques, and real-world applications.

[ğŸ“š What's Inside](#whats-inside) â€¢ [ğŸ“ Learn](#learning-path) â€¢ [ğŸ’» Algorithms](#algorithms-overview) â€¢ [ğŸ“Š Resources](#resources)

</div>

---

## ğŸ“– Overview

Unsupervised Learning is a powerful machine learning paradigm where models learn patterns, structures, and relationships directly from unlabeled data. Unlike supervised learning, there's **no predefined target variable** â€” instead, algorithms discover hidden patterns and groupings on their own.

This course provides a comprehensive exploration of unsupervised learning techniques, from foundational clustering algorithms to advanced dimensionality reduction methods.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        UNSUPERVISED LEARNING LANDSCAPE                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ ğŸ¯ GOAL: Find Hidden Patterns in Unlabeled Data        â”‚
â”‚                                                         â”‚
â”‚  â”œâ”€ Clustering      â†’ Group similar data points         â”‚
â”‚  â”œâ”€ Dimensionality  â†’ Reduce feature complexity         â”‚
â”‚  â”‚  Reduction                                           â”‚
â”‚  â””â”€ Association     â†’ Discover relationships            â”‚
â”‚     Rules                                               â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ What's Inside

This repository contains:

| ğŸ“‚ Component | ğŸ“ Description | ğŸ”§ Difficulty |
|:---|:---|:---:|
| **Clustering Algorithms** | K-Means, Hierarchical, DBSCAN, GMM | â­â­â­ |
| **Dimensionality Reduction** | PCA, t-SNE, UMAP, Autoencoders | â­â­â­â­ |
| **Association Rules** | Apriori, Eclat, Market Basket Analysis | â­â­ |
| **Anomaly Detection** | Isolation Forest, LOF, Statistical Methods | â­â­â­ |
| **Practical Projects** | Real-world datasets & implementations | â­â­â­â­ |
| **Visualizations** | Interactive plots & 3D representations | â­â­ |

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/MuhammadZafran33/Data-Science-Course.git

# Navigate to Unsupervised Learning folder
cd "Data-Science-Course/Data Science Full Course By WsCube Tech/Machine Learning Course/Unsupervised Learning"

# Install required packages
pip install -r requirements.txt
```

### Required Libraries

```python
# Core Data Science
numpy                # Numerical computing
pandas               # Data manipulation
scikit-learn         # ML algorithms
matplotlib           # Visualization
seaborn              # Statistical visualization

# Advanced Techniques
umap-learn           # Dimensionality reduction
plotly               # Interactive plots
scipy                # Scientific computing
tensorflow/keras     # Deep learning (autoencoders)
```

---

## ğŸ§  Algorithms Overview

### 1ï¸âƒ£ CLUSTERING ALGORITHMS

Clustering is the task of grouping similar data points together without predefined labels.

#### Algorithm Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLUSTERING ALGORITHMS                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  K-MEANS CLUSTERING        HIERARCHICAL CLUSTERING             â”‚
â”‚  â”œâ”€ Time: O(nÂ·kÂ·i)         â”œâ”€ Time: O(nÂ² to nÂ³)                â”‚
â”‚  â”œâ”€ Space: O(nÂ·k)          â”œâ”€ Space: O(nÂ²)                     â”‚
â”‚  â”œâ”€ Scalability: High       â”œâ”€ Scalability: Low                â”‚
â”‚  â”œâ”€ Clusters: Spherical     â”œâ”€ Clusters: Dendrogram            â”‚
â”‚  â””â”€ Best for: Large data    â””â”€ Best for: Small data            â”‚
â”‚                                                                â”‚
â”‚  DBSCAN                    GAUSSIAN MIXTURE MODELS             â”‚
â”‚  â”œâ”€ Time: O(n log n)       â”œâ”€ Time: O(nÂ·kÂ·i)                   â”‚
â”‚  â”œâ”€ Space: O(n)            â”œâ”€ Space: O(nÂ·k)                    â”‚
â”‚  â”œâ”€ Scalability: Medium     â”œâ”€ Scalability: Medium             â”‚
â”‚  â”œâ”€ Clusters: Any shape     â”œâ”€ Clusters: Probabilistic         â”‚
â”‚  â””â”€ Best for: Arbitrary     â””â”€ Best for: Soft clustering       â”‚
â”‚     density shapes                                             â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Detailed Comparison Table

| Algorithm | Time Complexity | Scalability | Shape | Interpretability | Use Case |
|:---|:---:|:---:|:---:|:---:|:---|
| **K-Means** | O(nki) | â­â­â­â­â­ | Spherical | â­â­â­â­ | Large datasets, e-commerce |
| **Hierarchical** | O(nÂ²) to O(nÂ³) | â­â­ | Any | â­â­â­â­â­ | Gene analysis, taxonomy |
| **DBSCAN** | O(n log n) | â­â­â­â­ | Any | â­â­â­ | Anomaly detection, spatial data |
| **GMM** | O(nki) | â­â­â­ | Elliptical | â­â­â­â­ | Soft clustering, generative models |
| **Spectral** | O(nÂ³) | â­ | Any | â­â­â­ | Image segmentation, non-convex clusters |

---

### 2ï¸âƒ£ DIMENSIONALITY REDUCTION

Reduce data complexity while preserving important information.

#### Method Comparison

```
DIMENSIONALITY REDUCTION TECHNIQUES
â”‚
â”œâ”€â”€â”€ LINEAR METHODS
â”‚    â”œâ”€ PCA (Principal Component Analysis)
â”‚    â”‚  â””â”€ Preserves variance, orthogonal components
â”‚    â”‚     â±ï¸  Speed: Fast | ğŸ“Š Interpretability: High
â”‚    â”‚
â”‚    â”œâ”€ ICA (Independent Component Analysis)
â”‚    â”‚  â””â”€ Finds independent components
â”‚    â”‚     â±ï¸  Speed: Moderate | ğŸ“Š Interpretability: Medium
â”‚    â”‚
â”‚    â””â”€ NMF (Non-Negative Matrix Factorization)
â”‚       â””â”€ For non-negative data
â”‚          â±ï¸  Speed: Fast | ğŸ“Š Interpretability: High
â”‚
â”œâ”€â”€â”€ NON-LINEAR METHODS
â”‚    â”œâ”€ t-SNE (t-Distributed Stochastic Neighbor Embedding)
â”‚    â”‚  â””â”€ Excellent for 2D/3D visualization
â”‚    â”‚     â±ï¸  Speed: Slow | ğŸ¨ Visualization: Excellent
â”‚    â”‚
â”‚    â”œâ”€ UMAP (Uniform Manifold Approximation & Projection)
â”‚    â”‚  â””â”€ Faster t-SNE alternative, preserves structure
â”‚    â”‚     â±ï¸  Speed: Fast | ğŸ¨ Visualization: Excellent
â”‚    â”‚
â”‚    â””â”€ Autoencoders (Deep Learning)
â”‚       â””â”€ Neural network-based compression
â”‚          â±ï¸  Speed: Depends | ğŸ“Š Power: Excellent
â”‚
â””â”€â”€â”€ MANIFOLD LEARNING
     â”œâ”€ Isomap
     â”œâ”€ Locally Linear Embedding (LLE)
     â””â”€ Laplacian Eigenmaps
```

#### Performance Metrics

| Technique | Speed | Interpretability | Non-linear | Preserve Global | Best For |
|:---|:---:|:---:|:---:|:---:|:---|
| **PCA** | âš¡âš¡âš¡âš¡âš¡ | â­â­â­â­ | âŒ | âœ… | Quick analysis, preprocessing |
| **t-SNE** | âš¡âš¡ | â­â­ | âœ… | âŒ | Visualization, exploration |
| **UMAP** | âš¡âš¡âš¡âš¡ | â­â­â­ | âœ… | âœ… | Visualization, large datasets |
| **ICA** | âš¡âš¡âš¡ | â­â­â­ | âŒ | âœ… | Blind source separation |
| **Autoencoders** | âš¡ | â­â­ | âœ… | âœ… | Complex patterns, deep learning |

---

### 3ï¸âƒ£ ANOMALY DETECTION

Identify unusual patterns and outliers in data.

```
ANOMALY DETECTION APPROACHES
â”‚
â”œâ”€ STATISTICAL METHODS
â”‚  â”œâ”€ Z-Score: Distance from mean
â”‚  â”œâ”€ IQR Method: Interquartile range
â”‚  â””â”€ Mahalanobis Distance: Multi-dimensional distance
â”‚
â”œâ”€ PROXIMITY-BASED
â”‚  â”œâ”€ Isolation Forest
â”‚  â”‚  â””â”€ Randomly isolates anomalies
â”‚  â”‚
â”‚  â”œâ”€ Local Outlier Factor (LOF)
â”‚  â”‚  â””â”€ Compares local density
â”‚  â”‚
â”‚  â””â”€ K-Nearest Neighbors (KNN)
â”‚     â””â”€ Based on neighbor distances
â”‚
â”œâ”€ MODEL-BASED
â”‚  â”œâ”€ One-Class SVM
â”‚  â”‚  â””â”€ Learns boundary of normal data
â”‚  â”‚
â”‚  â””â”€ Autoencoders
â”‚     â””â”€ High reconstruction error = anomaly
â”‚
â””â”€ ENSEMBLE METHODS
   â””â”€ Combination of multiple techniques
```

---

### 4ï¸âƒ£ ASSOCIATION RULES & MARKET BASKET ANALYSIS

Discover relationships between variables in transaction data.

| Algorithm | Approach | Time | Memory | Best For |
|:---|:---|:---:|:---:|:---|
| **Apriori** | Bottom-up, candidate generation | O(2^n) | High | Small-medium datasets |
| **Eclat** | Depth-first search, vertical format | O(2^n) | Medium | Finding frequent itemsets |
| **FP-Growth** | Prefix tree, pattern growth | O(n log n) | Low | Large datasets |

---

## ğŸ“Š Visual Learning Paths

### Beginner Path

```
START
  â”‚
  â”œâ”€â†’ Understand Clustering Concepts
  â”‚    â””â”€â†’ K-Means Clustering (Iris, Wine datasets)
  â”‚         â””â”€â†’ Visualize clusters in 2D/3D
  â”‚
  â”œâ”€â†’ Dimensionality Reduction
  â”‚    â””â”€â†’ PCA (Principal Component Analysis)
  â”‚         â””â”€â†’ Reduce high-dimensional data
  â”‚
  â””â”€â†’ Project: Customer Segmentation
       â””â”€â†’ Apply K-Means + PCA on real data
```

### Intermediate Path

```
START
  â”‚
  â”œâ”€â†’ Advanced Clustering
  â”‚    â”œâ”€â†’ Hierarchical Clustering (Dendrograms)
  â”‚    â”œâ”€â†’ DBSCAN (Density-based)
  â”‚    â””â”€â†’ Gaussian Mixture Models
  â”‚
  â”œâ”€â†’ Non-linear Dimensionality Reduction
  â”‚    â”œâ”€â†’ t-SNE for visualization
  â”‚    â””â”€â†’ UMAP for large datasets
  â”‚
  â”œâ”€â†’ Anomaly Detection
  â”‚    â”œâ”€â†’ Isolation Forest
  â”‚    â””â”€â†’ Local Outlier Factor
  â”‚
  â””â”€â†’ Project: Fraud Detection System
       â””â”€â†’ Detect anomalies in transaction data
```

### Advanced Path

```
START
  â”‚
  â”œâ”€â†’ Deep Unsupervised Learning
  â”‚    â”œâ”€â†’ Autoencoders
  â”‚    â”œâ”€â†’ Variational Autoencoders (VAE)
  â”‚    â””â”€â†’ Generative Adversarial Networks (GAN)
  â”‚
  â”œâ”€â†’ Advanced Association Rules
  â”‚    â”œâ”€â†’ FP-Growth Algorithm
  â”‚    â””â”€â†’ Sequential Pattern Mining
  â”‚
  â”œâ”€â†’ Manifold Learning
  â”‚    â”œâ”€â†’ Isomap
  â”‚    â”œâ”€â†’ LLE (Locally Linear Embedding)
  â”‚    â””â”€â†’ Spectral Clustering
  â”‚
  â””â”€â†’ Project: Advanced Data Discovery
       â””â”€â†’ Multi-algorithm ensemble approach
```

---

## ğŸ“ Learning Path

### Module 1: Clustering Fundamentals â­â­
- [ ] Introduction to clustering
- [ ] K-Means algorithm from scratch
- [ ] Elbow method & silhouette score
- [ ] Practical: Iris & Wine datasets
- [ ] **Duration:** 3-4 hours | **Difficulty:** Beginner

### Module 2: Advanced Clustering â­â­â­
- [ ] Hierarchical clustering
- [ ] DBSCAN algorithm
- [ ] Gaussian Mixture Models
- [ ] Choosing the right algorithm
- [ ] **Duration:** 5-6 hours | **Difficulty:** Intermediate

### Module 3: Dimensionality Reduction â­â­â­
- [ ] PCA concepts & implementation
- [ ] t-SNE & UMAP
- [ ] Feature extraction vs selection
- [ ] Practical: High-dimensional datasets
- [ ] **Duration:** 4-5 hours | **Difficulty:** Intermediate

### Module 4: Anomaly Detection â­â­â­
- [ ] Statistical methods
- [ ] Isolation Forest
- [ ] Local Outlier Factor
- [ ] Real-world applications
- [ ] **Duration:** 4-5 hours | **Difficulty:** Intermediate

### Module 5: Deep Unsupervised Learning â­â­â­â­
- [ ] Autoencoders
- [ ] Variational Autoencoders
- [ ] Generative models
- [ ] Applications in industry
- [ ] **Duration:** 6-8 hours | **Difficulty:** Advanced

### Module 6: Association Rules & Mining â­â­â­
- [ ] Market basket analysis
- [ ] Apriori algorithm
- [ ] FP-Growth
- [ ] Business applications
- [ ] **Duration:** 3-4 hours | **Difficulty:** Intermediate

---

## ğŸ’» Implementation Examples

### 1. K-Means Clustering

```python
from sklearn.cluster import KMeans
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

# Load data
iris = load_iris()
X = iris.data

# Apply K-Means
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X)

# Visualize
plt.scatter(X[:, 0], X[:, 1], c=labels)
plt.show()
```

### 2. PCA for Dimensionality Reduction

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris

iris = load_iris()
X = iris.data

# Reduce to 2 dimensions
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

# Explained variance
print(f"Explained variance: {pca.explained_variance_ratio_}")
```

### 3. Anomaly Detection with Isolation Forest

```python
from sklearn.ensemble import IsolationForest
import numpy as np

X = np.random.randn(100, 2)
X = np.vstack([X, [10, 10]])  # Add outlier

clf = IsolationForest(contamination=0.01)
outliers = clf.fit_predict(X)
```

---

## ğŸ“ˆ Algorithm Effectiveness Chart

```
Performance Across Different Scenarios
â”‚
â”‚ 100% â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚      â”‚
â”‚  80% â”œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚      â”‚ K-Mâ”‚ Hierâ”‚
â”‚      â”‚    â”‚    â”‚
â”‚  60% â”œâ”€â”€â”€â”€â”¼â”€â”€â”¬â”€â”€â”¤â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚      â”‚    â”‚GMâ”‚D â”‚tSNEâ”‚
â”‚      â”‚    â”‚M â”‚B â”‚    â”‚
â”‚  40% â”œâ”€â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚      â”‚    â”‚  â”‚  â”‚ PCAâ”‚UMAPâ”‚
â”‚      â”‚    â”‚  â”‚  â”‚    â”‚    â”‚
â”‚  20% â”œâ”€â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚      â”‚    â”‚  â”‚  â”‚    â”‚    â”‚AE â”‚
â”‚      â”‚    â”‚  â”‚  â”‚    â”‚    â”‚   â”‚
â”‚   0% â””â”€â”€â”€â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚      Spherical Non-linear Complex Non-euclidean
â”‚      Clusters  Patterns   Patterns  Spaces
â”‚
Legend:
K-M    = K-Means
Hier   = Hierarchical
GMM    = Gaussian Mixture Models
DBSCAN = DBSCAN
PCA    = Principal Component Analysis
t-SNE  = t-Distributed SNE
UMAP   = Uniform Manifold Approximation
AE     = Autoencoder
```

---

## ğŸ“ Folder Structure

```
Unsupervised Learning/
â”‚
â”œâ”€â”€ 1_Clustering/
â”‚   â”œâ”€â”€ 1_K_Means.ipynb
â”‚   â”œâ”€â”€ 2_Hierarchical_Clustering.ipynb
â”‚   â”œâ”€â”€ 3_DBSCAN.ipynb
â”‚   â”œâ”€â”€ 4_Gaussian_Mixture_Models.ipynb
â”‚   â””â”€â”€ datasets/
â”‚
â”œâ”€â”€ 2_Dimensionality_Reduction/
â”‚   â”œâ”€â”€ 1_PCA.ipynb
â”‚   â”œâ”€â”€ 2_t_SNE.ipynb
â”‚   â”œâ”€â”€ 3_UMAP.ipynb
â”‚   â”œâ”€â”€ 4_Autoencoders.ipynb
â”‚   â””â”€â”€ visualizations/
â”‚
â”œâ”€â”€ 3_Anomaly_Detection/
â”‚   â”œâ”€â”€ 1_Statistical_Methods.ipynb
â”‚   â”œâ”€â”€ 2_Isolation_Forest.ipynb
â”‚   â”œâ”€â”€ 3_Local_Outlier_Factor.ipynb
â”‚   â””â”€â”€ datasets/
â”‚
â”œâ”€â”€ 4_Association_Rules/
â”‚   â”œâ”€â”€ 1_Apriori.ipynb
â”‚   â”œâ”€â”€ 2_Eclat.ipynb
â”‚   â””â”€â”€ market_basket_analysis.ipynb
â”‚
â”œâ”€â”€ 5_Real_World_Projects/
â”‚   â”œâ”€â”€ Customer_Segmentation.ipynb
â”‚   â”œâ”€â”€ Fraud_Detection.ipynb
â”‚   â””â”€â”€ Image_Compression.ipynb
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ¯ Key Concepts Summary

| Concept | What It Does | When To Use | Pros | Cons |
|:---|:---|:---|:---|:---|
| **K-Means** | Partitions data into k clusters | Fixed, balanced clusters | Fast, scalable | Requires k specification |
| **Hierarchical** | Creates cluster dendrogram | Tree-like structures | Visual, interpretable | Computationally expensive |
| **DBSCAN** | Density-based clustering | Arbitrary shapes, outliers | Finds anomalies, flexible | Parameter tuning needed |
| **PCA** | Linear dimensionality reduction | Quick analysis, preprocessing | Fast, orthogonal | Loses non-linear info |
| **t-SNE** | Non-linear visualization | 2D/3D exploration | Beautiful visualizations | Slow, loses global structure |
| **UMAP** | Fast non-linear reduction | Large datasets, visualization | Fast, preserves structure | Newer, less studied |
| **Autoencoders** | Deep neural compression | Complex patterns | Very flexible, powerful | Requires large data |
| **Isolation Forest** | Anomaly detection | Outlier identification | Works with any data | Limited interpretability |

---

## ğŸ”— Resources & References

### ğŸ“š Recommended Books
- "Pattern Recognition and Machine Learning" - Christopher M. Bishop
- "Unsupervised Learning" - Ethem Alpaydin
- "Machine Learning: A Probabilistic Perspective" - Kevin P. Murphy

### ğŸŒ Online Resources
- [Scikit-Learn Documentation](https://scikit-learn.org/)
- [Andrew Ng's ML Course](https://www.coursera.org/learn/machine-learning)
- [Fast.ai Deep Learning](https://www.fast.ai/)

### ğŸ“° Research Papers
- "A Tutorial on Clustering" - Andrew Moore
- "The Art and Science of Tuning Machine Learning Algorithms" - Limsoon Wong
- "UMAP: Uniform Manifold Approximation and Projection" - Leland McInnes

### ğŸ¥ YouTube Channels
- [StatQuest with Josh Starmer](https://www.youtube.com/@statquest)
- [3Blue1Brown](https://www.youtube.com/@3blue1brown)
- [Code Basics](https://www.youtube.com/@codebasics)

---

## ğŸ“Š Comparison Matrix

### Algorithm Selection Guide

```
CHOOSE K-MEANS IF:
âœ“ You need fast clustering
âœ“ Clusters are roughly spherical
âœ“ You know the number of clusters
âœ“ Working with large datasets
âœ— Clusters have irregular shapes

CHOOSE HIERARCHICAL IF:
âœ“ Need interpretable dendrogram
âœ“ Clusters are nested
âœ“ Small to medium dataset
âœ“ Want to explore different k
âœ— Scalability is a concern

CHOOSE DBSCAN IF:
âœ“ Clusters are arbitrary shape
âœ“ Need to detect outliers
âœ“ Density varies across space
âœ“ Don't know number of clusters
âœ— High-dimensional data

CHOOSE PCA IF:
âœ“ Need quick dimensionality reduction
âœ“ Interpretability is important
âœ“ Linear relationships exist
âœ“ Large dataset
âœ— Need to preserve non-linear structure

CHOOSE t-SNE IF:
âœ“ Creating exploratory visualizations
âœ“ Want beautiful 2D/3D plots
âœ“ Cluster separation is important
âœ— Need to preserve global structure
âœ— Have very large dataset (slow)

CHOOSE UMAP IF:
âœ“ Need to preserve both local and global structure
âœ“ Visualization with larger datasets
âœ“ Want faster than t-SNE
âœ“ Scalability is important
```

---

## ğŸš€ Quick Tips & Best Practices

### Data Preprocessing
- âœ… Always scale/normalize features (StandardScaler, MinMaxScaler)
- âœ… Handle missing values before clustering
- âœ… Remove or handle outliers based on context
- âœ… Feature selection can improve results

### Choosing K (Number of Clusters)
- ğŸ“Š **Elbow Method**: Plot inertia vs k, look for "elbow"
- ğŸ“Š **Silhouette Score**: Closer to 1 is better
- ğŸ“Š **Davies-Bouldin Index**: Lower is better
- ğŸ“Š **Domain Knowledge**: Use business requirements

### Evaluation Metrics
- **Silhouette Score**: -1 to 1 (higher is better)
- **Davies-Bouldin Index**: Lower is better
- **Calinski-Harabasz Index**: Higher is better
- **Homogeneity, Completeness, V-measure** (when labels known)

### Common Pitfalls
- âŒ Not scaling features before clustering
- âŒ Using k-means for non-spherical clusters
- âŒ Forgetting to validate results
- âŒ Ignoring computational complexity
- âŒ Not visualizing results

---

## ğŸ Bonus: Interactive Decision Tree

```
START: Choose Your Algorithm
â”‚
â””â”€ What's your goal?
   â”‚
   â”œâ”€ GROUPING DATA
   â”‚  â””â”€ Do you know clusters shape?
   â”‚     â”œâ”€ YES, spherical
   â”‚     â”‚  â””â”€â†’ K-MEANS â­â­â­
   â”‚     â”‚
   â”‚     â”œâ”€ NO, arbitrary shape
   â”‚     â”‚  â””â”€â†’ DBSCAN â­â­â­â­
   â”‚     â”‚
   â”‚     â””â”€ Want tree structure?
   â”‚        â””â”€â†’ HIERARCHICAL â­â­â­
   â”‚
   â”œâ”€ REDUCING DIMENSIONS
   â”‚  â””â”€ Need interpretability?
   â”‚     â”œâ”€ YES
   â”‚     â”‚  â””â”€â†’ PCA â­â­â­â­
   â”‚     â”‚
   â”‚     â”œâ”€ NO, want visualization
   â”‚     â”‚  â”œâ”€ Speed important?
   â”‚     â”‚  â”‚  â”œâ”€ YES
   â”‚     â”‚  â”‚  â”‚  â””â”€â†’ UMAP â­â­â­â­
   â”‚     â”‚  â”‚  â”‚
   â”‚     â”‚  â”‚  â””â”€ NO, beautiful plots
   â”‚     â”‚  â”‚     â””â”€â†’ t-SNE â­â­â­â­
   â”‚     â”‚  â”‚
   â”‚     â”‚  â””â”€ Complex patterns?
   â”‚     â”‚     â””â”€â†’ AUTOENCODER â­â­â­â­
   â”‚
   â””â”€ FINDING OUTLIERS
      â””â”€â†’ ISOLATION FOREST â­â­â­

```

---

## ğŸ“ Support & Contribution

### Questions? Issues?
- ğŸ“§ Open an issue on GitHub
- ğŸ’¬ Check existing discussions
- ğŸ“– Review notebook comments for explanations

### Want to Contribute?
- Fork the repository
- Add improvements or new algorithms
- Submit pull requests
- Share your projects!

### Code Standards
- Clear, commented code
- Docstrings for functions
- Example notebooks for implementations
- README updates for new content

---

## ğŸ“œ License & Attribution

This course material is based on the WsCube Tech Machine Learning curriculum, with enhancements and practical implementations.

**Created with â¤ï¸ for Data Science Enthusiasts**

---

## ğŸ¯ Your Learning Journey

```
Week 1-2: Understand Clustering
â”œâ”€ Concepts
â”œâ”€ K-Means
â””â”€ Evaluation

Week 3-4: Advanced Clustering & PCA
â”œâ”€ Hierarchical & DBSCAN
â”œâ”€ Dimensionality Reduction
â””â”€ Visualization

Week 5-6: Anomaly Detection & Deep Learning
â”œâ”€ Outlier Detection
â”œâ”€ Autoencoders
â””â”€ Advanced Applications

Week 7-8: Real-World Projects
â”œâ”€ Customer Segmentation
â”œâ”€ Fraud Detection
â””â”€ Capstone Project

ğŸ† MASTER UNSUPERVISED LEARNING! ğŸ†
```

---

<div align="center">

### **Happy Learning! ğŸš€**

> "Data is talking. Unsupervised learning helps you listen." - Unknown

**Star â­ this repository if you found it helpful!**

[![Made with â¤ï¸](https://img.shields.io/badge/Made%20with-â¤ï¸-red?style=flat-square)](https://github.com/MuhammadZafran33)

</div>

---

*Last Updated: February 2026 | WsCube Tech ML Course*
