<div align="center">

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     HEADER  â€”  100 % GitHub-safe
     â€¢ readme-typing-svg.demolab.com   âœ… always renders
     â€¢ img.shields.io badges           âœ… always renders
     â€¢ user-images.githubusercontent.com animated gif âœ…
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->

<img src="https://readme-typing-svg.demolab.com?font=Fira+Code&weight=800&size=44&duration=2800&pause=900&color=10B981&center=true&vCenter=true&width=1000&height=95&lines=%F0%9F%A4%96+MACHINE+LEARNING+COURSE;Supervised+%7C+Unsupervised+%7C+Ensemble;From+Linear+Regression+to+XGBoost+%F0%9F%9A%80" alt="ML Header"/>

<img src="https://readme-typing-svg.demolab.com?font=Fira+Code&weight=400&size=18&duration=4000&pause=1500&color=6EE7B7&center=true&vCenter=true&width=1000&height=45&lines=WsCube+Tech+Data+Science+Course+%7C+Muhammad+Zafran;14+Modules+%7C+30%2B+Algorithms+%7C+10%2B+Real-World+Projects" alt="Subtitle"/>

<br/>
<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%"/>
<br/>

<!-- â”€â”€ BADGE ROW 1 : Core Stack â”€â”€ -->
<p>
  <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white"/>
  <img src="https://img.shields.io/badge/Scikit--Learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white"/>
  <img src="https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white"/>
  <img src="https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white"/>
  <img src="https://img.shields.io/badge/Jupyter-F37626?style=for-the-badge&logo=jupyter&logoColor=white"/>
</p>

<!-- â”€â”€ BADGE ROW 2 : Algorithms â”€â”€ -->
<p>
  <img src="https://img.shields.io/badge/Linear%20Regression-10B981?style=flat-square"/>
  <img src="https://img.shields.io/badge/Logistic%20Regression-059669?style=flat-square"/>
  <img src="https://img.shields.io/badge/Decision%20Tree-6EE7B7?style=flat-square"/>
  <img src="https://img.shields.io/badge/Random%20Forest-047857?style=flat-square"/>
  <img src="https://img.shields.io/badge/SVM-065F46?style=flat-square"/>
  <img src="https://img.shields.io/badge/XGBoost-10B981?style=flat-square"/>
  <img src="https://img.shields.io/badge/KNN-6EE7B7?style=flat-square"/>
  <img src="https://img.shields.io/badge/K--Means-059669?style=flat-square"/>
  <img src="https://img.shields.io/badge/PCA-047857?style=flat-square"/>
</p>

<!-- â”€â”€ BADGE ROW 3 : Repo Meta â”€â”€ -->
<p>
  <img src="https://img.shields.io/badge/Module-Machine%20Learning-10B981?style=flat-square"/>
  <img src="https://img.shields.io/badge/Course-WsCube%20Tech-F7931E?style=flat-square&logo=youtube&logoColor=white"/>
  <img src="https://img.shields.io/badge/Level-Intermediate%20%E2%86%92%20Advanced-orange?style=flat-square"/>
  <img src="https://img.shields.io/badge/Notebooks-25%2B-success?style=flat-square"/>
  <img src="https://img.shields.io/badge/Algorithms-30%2B-brightgreen?style=flat-square"/>
  <img src="https://img.shields.io/github/last-commit/MuhammadZafran33/Data-Science-Course?style=flat-square&color=10B981"/>
</p>

<br/>

> ### ğŸ¤– *"Machine Learning is the science of getting computers to act without being explicitly programmed."*
> **â€” Andrew Ng**

<br/>

</div>

---

## ğŸ“š Table of Contents

| # | Section | Quick Jump |
|---|---------|-----------|
| 01 | ğŸ—ºï¸ Module Overview | [Jump](#ï¸-module-overview) |
| 02 | ğŸ§­ Full Learning Roadmap | [Jump](#-full-learning-roadmap) |
| 03 | ğŸ“¦ All 14 Modules â€” Deep Dive | [Jump](#-all-14-modules--deep-dive) |
| 04 | ğŸ“Š Coverage & Analytics Charts | [Jump](#-coverage--analytics-charts) |
| 05 | ğŸ”€ Algorithm Selector | [Jump](#-which-algorithm-should-i-use) |
| 06 | ğŸ“ Regression vs Classification | [Jump](#-regression-vs-classification) |
| 07 | ğŸŒ² Ensemble Methods Guide | [Jump](#-ensemble-methods-guide) |
| 08 | ğŸ”µ Clustering Algorithms | [Jump](#-clustering-algorithms) |
| 09 | ğŸ“ All Metrics Cheatsheet | [Jump](#-metrics-cheatsheet) |
| 10 | âš™ï¸ Feature Engineering Pipeline | [Jump](#ï¸-feature-engineering-pipeline) |
| 11 | ğŸ’» Quick Code Reference | [Jump](#-quick-code-reference) |
| 12 | ğŸ“ Folder Structure | [Jump](#-folder-structure) |
| 13 | ğŸ† Projects Showcase | [Jump](#-projects-showcase) |
| 14 | ğŸš€ Getting Started | [Jump](#-getting-started) |
| 15 | ğŸ”— Navigation | [Jump](#-navigation) |

---

## ğŸ—ºï¸ Module Overview

<div align="center">

| ğŸ“Œ Attribute | ğŸ“‹ Details |
|-------------|-----------|
| ğŸ“ **Parent Course** | Data Science Full Course â€” WsCube Tech |
| ğŸ“‚ **Module Name** | Machine Learning Course |
| ğŸ“ **Course Position** | Modules 21â€“34 (after EDA, before Deep Learning) |
| â±ï¸ **Study Duration** | 6â€“8 Weeks Â· 60+ Hours |
| ğŸ““ **Notebooks** | 25+ Jupyter Notebooks |
| ğŸ¤– **Algorithms Covered** | 30+ algorithms across Supervised & Unsupervised learning |
| ğŸ”§ **Primary Library** | Scikit-learn Â· XGBoost Â· StatsModels |
| ğŸ† **Projects** | 10+ hands-on real-world projects |
| ğŸ¯ **Career Roles** | ML Engineer Â· Data Scientist Â· AI Developer |

</div>

---

## ğŸ§­ Full Learning Roadmap

```mermaid
flowchart TD
    START(["ğŸš€ START\nMachine Learning"])

    START --> A["ğŸ“– ML Foundations\nMod 21 â€” What is ML?\nSupervised vs Unsupervised\nML Workflow & Pipeline"]

    A --> B["âš™ï¸ Feature Engineering\nMod 22 â€” Data Cleaning\nEncoding Â· Scaling\nOutlier Removal Â· Feature Selection"]

    B --> SUP

    subgraph SUP ["ğŸ¯ SUPERVISED LEARNING  (Mods 23â€“30)"]
        direction LR
        REG["ğŸ“ˆ Regression\nLinear Â· Multiple\nOLS Â· Regularization"]
        CLS["ğŸ·ï¸ Classification\nLogistic Â· SVM Â· KNN\nNaive Bayes Â· Trees"]
        ENS["ğŸŒ² Ensemble Methods\nRandom Forest\nAdaBoost Â· XGBoost"]
        TUN["ğŸ›ï¸ Tuning\nCross-Validation\nGridSearch Â· GD"]
        REG --> CLS --> ENS --> TUN
    end

    TUN --> UNSUP

    subgraph UNSUP ["ğŸ”µ UNSUPERVISED LEARNING  (Mods 31â€“34)"]
        direction LR
        CLU["ğŸ”µ Clustering\nK-Means Â· DBSCAN\nHierarchical"]
        ASS["ğŸ›’ Association Rules\nApriori Â· FP-Growth\nMarket Basket"]
        DIM["ğŸ“‰ Dimensionality\nPCA Â· Eigen Vectors\nVariance Explained"]
        CLU --> ASS --> DIM
    end

    UNSUP --> END(["âœ… COMPLETE\nReady for Deep Learning"])

    style START fill:#10B981,stroke:none,color:#000
    style END   fill:#059669,stroke:none,color:#fff
    style A     fill:#022c22,stroke:#10B981,color:#6EE7B7
    style B     fill:#022c22,stroke:#10B981,color:#6EE7B7
    style SUP   fill:#064e3b,stroke:#10B981,color:#6EE7B7
    style UNSUP fill:#064e3b,stroke:#6EE7B7,color:#6EE7B7
    style REG   fill:#065f46,stroke:none,color:#a7f3d0
    style CLS   fill:#065f46,stroke:none,color:#a7f3d0
    style ENS   fill:#047857,stroke:none,color:#a7f3d0
    style TUN   fill:#047857,stroke:none,color:#a7f3d0
    style CLU   fill:#065f46,stroke:none,color:#a7f3d0
    style ASS   fill:#065f46,stroke:none,color:#a7f3d0
    style DIM   fill:#047857,stroke:none,color:#a7f3d0
```

---

## ğŸ“¦ All 14 Modules â€” Deep Dive

### ğŸŸ¢ Module 21 â€” Introduction to Machine Learning

| Topic | Description | Notebook |
|-------|-------------|:--------:|
| What is ML? | Definition, types, real-world applications | `01_intro_ml.ipynb` |
| ML vs Traditional Programming | Rule-based vs data-driven learning | `01_intro_ml.ipynb` |
| Supervised Learning | Learning from labeled data (X â†’ y) | `01_intro_ml.ipynb` |
| Unsupervised Learning | Finding patterns in unlabeled data | `01_intro_ml.ipynb` |
| Reinforcement Learning | Reward-based learning agents | `01_intro_ml.ipynb` |
| The ML Workflow | Data â†’ Features â†’ Model â†’ Evaluate â†’ Deploy | `01_intro_ml.ipynb` |

---

### ğŸŸ¢ Module 22 â€” Feature Engineering

| Topic | Key Techniques | Notebook |
|-------|---------------|:--------:|
| **Data Cleaning** | Remove/fill nulls, fix dtypes | `02_feature_engineering.ipynb` |
| **Feature Selection** | Correlation heatmap Â· Backward/Forward Elimination | `02_feature_engineering.ipynb` |
| **Encoding Categoricals** | One-Hot Â· Dummy Â· Label Â· Ordinal Encoding | `02_feature_engineering.ipynb` |
| **Outlier Removal** | IQR method Â· Z-score Â· Isolation Forest | `02_feature_engineering.ipynb` |
| **Scaling** | StandardScaler Â· MinMaxScaler Â· RobustScaler | `02_feature_engineering.ipynb` |
| **Train-Test Split** | `train_test_split()` Â· stratification | `02_feature_engineering.ipynb` |
| **Bias-Variance** | Overfitting vs underfitting trade-off | `02_feature_engineering.ipynb` |

---

### ğŸŸ¢ Modules 23â€“24 â€” Regression & Classification

#### ğŸ“ˆ Regression Analysis (Module 23)

| Algorithm | Key Formula | Metrics | Project | Notebook |
|-----------|------------|---------|---------|:--------:|
| **Simple Linear Reg.** | `y = mx + b` | RÂ², MAE, MSE, RMSE | â€” | `03_linear_regression.ipynb` |
| **Multiple Linear Reg.** | `y = bâ‚€ + bâ‚xâ‚ + â€¦ + bâ‚™xâ‚™` | Adjusted RÂ², RMSE | â€” | `03_linear_regression.ipynb` |
| **OLS (Ordinary Least Squares)** | Minimise `Î£(yâˆ’Å·)Â²` analytically | RÂ², p-values | â€” | `03_linear_regression.ipynb` |
| **Ridge (L2)** | OLS + `Î»Î£wáµ¢Â²` penalty | RÂ², RMSE | â€” | `03_regularization.ipynb` |
| **Lasso (L1)** | OLS + `Î»Î£|wáµ¢|` penalty (sparse) | RÂ², RMSE | â€” | `03_regularization.ipynb` |
| ğŸ—ï¸ **Project** | â€” | â€” | **Heating & Cooling Load Prediction** | `03_project_heating.ipynb` |

#### ğŸ·ï¸ Classification Analysis (Module 24)

| Algorithm | Decision Boundary | Output | Project | Notebook |
|-----------|-----------------|--------|---------|:--------:|
| **Logistic Regression** | Linear (sigmoid output) | P(class) âˆˆ (0,1) | â€” | `04_logistic_regression.ipynb` |
| **Confusion Matrix** | TP / FP / TN / FN breakdown | â€” | â€” | `04_logistic_regression.ipynb` |
| **Imbalanced Datasets** | SMOTE Â· class_weight Â· resampling | â€” | â€” | `04_logistic_regression.ipynb` |
| ğŸ—ï¸ **Project** | â€” | â€” | **Diabetic Patient Classification** | `04_project_diabetes.ipynb` |

---

### ğŸŸ¢ Modules 25â€“28 â€” Core Algorithms

#### ğŸ”µ Module 25 â€” NaÃ¯ve Bayes

| Topic | Detail | Notebook |
|-------|--------|:--------:|
| **Bayes' Theorem** | `P(A|B) = P(B|A)Â·P(A) / P(B)` | `05_naive_bayes.ipynb` |
| **Gaussian NB** | Continuous features, assumes normal dist. | `05_naive_bayes.ipynb` |
| **Multinomial NB** | Count features â€” best for text | `05_naive_bayes.ipynb` |
| **Bernoulli NB** | Binary features | `05_naive_bayes.ipynb` |
| ğŸ—ï¸ **Project** | **News Article Classification** | `05_project_news.ipynb` |

#### ğŸ”µ Module 26 â€” K-Nearest Neighbor (KNN)

| Topic | Detail | Notebook |
|-------|--------|:--------:|
| **How KNN Works** | Find K closest points â†’ majority vote | `06_knn.ipynb` |
| **Distance Metrics** | Euclidean Â· Manhattan Â· Minkowski | `06_knn.ipynb` |
| **Choosing K** | Elbow method on validation error | `06_knn.ipynb` |
| **KNN for Regression** | Average of K neighbors' values | `06_knn.ipynb` |
| ğŸ—ï¸ **Project** | **Social Network Ads Classification** | `06_project_social_ads.ipynb` |

#### ğŸ”µ Module 27 â€” Tree-Based Models

| Topic | Detail | Notebook |
|-------|--------|:--------:|
| **Decision Tree** | Recursive splits on best feature | `07_decision_tree.ipynb` |
| **Gini Impurity** | `1 âˆ’ Î£páµ¢Â²` â€” measures node impurity | `07_decision_tree.ipynb` |
| **Entropy / Info Gain** | `âˆ’Î£páµ¢logâ‚‚páµ¢` â€” information gain | `07_decision_tree.ipynb` |
| **Random Forest** | Bagging of N decision trees | `07_random_forest.ipynb` |
| **Feature Importance** | Mean decrease in impurity per feature | `07_random_forest.ipynb` |
| ğŸ—ï¸ **Project** | **Iris Flower Classification** | `07_project_iris.ipynb` |

#### ğŸ”µ Module 28 â€” Support Vector Machine (SVM)

| Topic | Detail | Notebook |
|-------|--------|:--------:|
| **Maximal Margin** | Hyperplane maximising margin between classes | `08_svm.ipynb` |
| **Support Vectors** | Data points closest to the decision boundary | `08_svm.ipynb` |
| **Kernel Trick** | Maps data to higher dimension â€” RBF Â· Poly Â· Linear | `08_svm.ipynb` |
| **C Parameter** | Regularization â€” soft vs hard margin | `08_svm.ipynb` |
| **SVR** | SVM for regression problems | `08_svm.ipynb` |

---

### ğŸŸ¢ Modules 29â€“30 â€” Ensemble Learning & Tuning

#### ğŸŒ² Module 29 â€” Ensemble Methods

| Method | Strategy | Base Learner | When to Use | Notebook |
|--------|---------|------------|------------|:--------:|
| **Bagging** | Train on bootstrap samples, aggregate | Any (usually DT) | High variance models | `09_ensemble.ipynb` |
| **Random Forest** | Bagging + random feature subsets | Decision Tree | Most tabular problems âœ… | `09_ensemble.ipynb` |
| **AdaBoost** | Sequential, re-weight misclassified | Weak learners | Imbalanced data | `09_adaboost.ipynb` |
| **XGBoost** | Gradient Boosting + regularization | Decision Tree | Kaggle competitions âœ… | `09_xgboost.ipynb` |
| **Gradient Boosting** | Each tree corrects predecessor's error | Decision Tree | Regression & classification | `09_xgboost.ipynb` |

#### ğŸ›ï¸ Module 30 â€” Model Tuning

| Topic | Detail | Notebook |
|-------|--------|:--------:|
| **Gradient Descent** | Batch Â· Stochastic (SGD) Â· Mini-batch | `10_gradient_descent.ipynb` |
| **K-Fold CV** | Split data into K folds, rotate validation | `10_cross_validation.ipynb` |
| **Stratified K-Fold** | Preserves class distribution across folds | `10_cross_validation.ipynb` |
| **GridSearchCV** | Exhaustive hyperparameter search | `10_hyperparameter_tuning.ipynb` |
| **RandomizedSearchCV** | Faster random parameter sampling | `10_hyperparameter_tuning.ipynb` |

---

### ğŸŸ¢ Modules 31â€“34 â€” Unsupervised Learning

#### ğŸ”µ Module 31 â€” K-Means Clustering

| Topic | Detail | Notebook |
|-------|--------|:--------:|
| **K-Means Algorithm** | Assign points to nearest centroid, recompute | `11_kmeans.ipynb` |
| **Elbow Method** | Plot inertia vs K â†’ find optimal clusters | `11_kmeans.ipynb` |
| **Silhouette Score** | Measure cluster cohesion and separation | `11_kmeans.ipynb` |
| ğŸ—ï¸ **Project** | **Shopping Dataset Clustering** | `11_project_shopping.ipynb` |

#### ğŸ”µ Module 32 â€” Hierarchical & DBSCAN Clustering

| Topic | Detail | Notebook |
|-------|--------|:--------:|
| **Agglomerative** | Bottom-up â€” merge closest clusters | `12_hierarchical.ipynb` |
| **Single/Complete Linkage** | Min vs max distance between clusters | `12_hierarchical.ipynb` |
| **Dendrogram** | Tree diagram â€” choose cut height for K | `12_hierarchical.ipynb` |
| **DBSCAN** | Density-based â€” finds arbitrary shapes, noise | `12_dbscan.ipynb` |
| **Epsilon & MinPts** | DBSCAN parameters: neighbourhood radius & min size | `12_dbscan.ipynb` |
| ğŸ—ï¸ **Project** | **Shopping Dataset â€” Advanced Clustering** | `12_project_clustering.ipynb` |

#### ğŸ”µ Module 33 â€” Association Rule Learning

| Topic | Detail | Notebook |
|-------|--------|:--------:|
| **Support** | `freq(AâˆªB) / N` â€” how often itemset appears | `13_association_rules.ipynb` |
| **Confidence** | `freq(AâˆªB) / freq(A)` â€” reliability of rule | `13_association_rules.ipynb` |
| **Lift** | `Confidence / Support(B)` â€” strength vs random | `13_association_rules.ipynb` |
| **Apriori** | Level-by-level candidate generation | `13_association_rules.ipynb` |
| **FP-Growth** | Frequent pattern tree â€” faster than Apriori | `13_association_rules.ipynb` |
| ğŸ—ï¸ **Project** | **Market Basket Analysis** | `13_project_basket.ipynb` |

#### ğŸ”µ Module 34 â€” Principal Component Analysis (PCA)

| Topic | Detail | Notebook |
|-------|--------|:--------:|
| **Dimensionality Reduction** | Project high-dim data to lower-dim space | `14_pca.ipynb` |
| **Eigen Values & Vectors** | Directions of max variance in data | `14_pca.ipynb` |
| **Explained Variance Ratio** | Choose n_components to keep 95% variance | `14_pca.ipynb` |
| **Scree Plot** | Visualise variance per component | `14_pca.ipynb` |
| **PCA + Classification** | Reduce features then feed to ML model | `14_pca.ipynb` |

---

## ğŸ“Š Coverage & Analytics Charts

### Content Distribution

```mermaid
pie title ML Course â€” Content Distribution by Hours
    "ML Foundations and Feature Engineering" : 12
    "Regression Analysis" : 10
    "Classification Algorithms" : 15
    "Tree Models and Ensemble" : 14
    "Model Tuning and Validation" : 10
    "Unsupervised Clustering" : 12
    "Association Rules" : 6
    "PCA and Dimensionality" : 6
    "Projects and Practice" : 15
```

### Study Hours Per Module

```mermaid
xychart-beta
    title "Study Hours Per Module"
    x-axis ["Intro ML", "Feature Eng", "Regression", "Classification", "Naive Bayes", "KNN", "Trees RF", "SVM", "Ensemble", "Tuning", "K-Means", "Hierarchical", "Assoc Rules", "PCA"]
    y-axis "Hours" 0 --> 7
    bar [3, 4, 5, 5, 3, 3, 6, 4, 6, 5, 4, 4, 3, 3]
```

### Module Timeline in Course

```mermaid
gantt
    title Machine Learning Position in WsCube Tech DS Course
    dateFormat  X
    axisFormat  Module %s

    section Pre-ML
    Python Foundations    :done,  f1, 1, 5
    Stats and Probability :done,  f2, 5, 7
    NumPy and Pandas      :done,  f3, 7, 10
    Visualization and EDA :done,  f4, 10, 13

    section Supervised Learning
    ML Intro and Feature Eng :active, s1, 13, 15
    Regression Analysis      :active, s2, 15, 17
    Classification           :        s3, 17, 19
    Naive Bayes and KNN      :        s4, 19, 21
    Trees and Ensemble       :        s5, 21, 24
    Tuning and Validation    :        s6, 24, 26

    section Unsupervised Learning
    K-Means Clustering       :  u1, 26, 28
    Hierarchical and DBSCAN  :  u2, 28, 29
    Association Rules        :  u3, 29, 30
    PCA                      :  u4, 30, 31

    section Deep Learning
    DL and Neural Networks   :  d1, 31, 36
```

### Algorithm Complexity Comparison

```mermaid
quadrantChart
    title Algorithm Complexity vs Performance Trade-off
    x-axis Low Complexity --> High Complexity
    y-axis Low Performance --> High Performance
    quadrant-1 High Power High Cost
    quadrant-2 Best Sweet Spot
    quadrant-3 Simple Baseline
    quadrant-4 Avoid
    Linear Regression: [0.15, 0.55]
    Logistic Regression: [0.18, 0.60]
    Naive Bayes: [0.12, 0.52]
    KNN: [0.30, 0.60]
    Decision Tree: [0.35, 0.65]
    SVM: [0.55, 0.75]
    Random Forest: [0.55, 0.85]
    AdaBoost: [0.60, 0.82]
    XGBoost: [0.72, 0.92]
    K-Means: [0.25, 0.58]
    PCA: [0.30, 0.62]
```

---

## ğŸ”€ Which Algorithm Should I Use?

```mermaid
flowchart TD
    Q(["ğŸ¤” What is your task?"]) --> A{"Labeled data?"}

    A -- "YES â€” Supervised" --> B{"Predict what?"}
    A -- "NO â€” Unsupervised" --> C{"Goal?"}

    B -- "Continuous number" --> REG{"How many features?"}
    B -- "Category / Class" --> CLS{"Dataset size?"}

    REG -- "1â€“2 features" --> LR["ğŸ“ˆ Linear Regression"]
    REG -- "Many features" --> MLR["ğŸ“ˆ Multiple / Ridge / Lasso"]

    CLS -- "Small, text data" --> NB["ğŸ”µ NaÃ¯ve Bayes"]
    CLS -- "Any size" --> D{"Interpretability?"}

    D -- "High needed" --> DT["ğŸŒ¿ Decision Tree\nor Logistic Reg"]
    D -- "Not critical" --> E{"More accuracy?"}

    E -- "Yes, balanced data" --> RF["ğŸŒ² Random Forest\nor SVM"]
    E -- "Maximum accuracy" --> XGB["ğŸ† XGBoost\nor AdaBoost"]

    C -- "Find groups" --> F{"Shape of clusters?"}
    C -- "Find patterns" --> ASS["ğŸ›’ Apriori / FP-Growth"]
    C -- "Reduce features" --> PCA_N["ğŸ“‰ PCA"]

    F -- "Spherical, known K" --> KM["ğŸ”µ K-Means"]
    F -- "Arbitrary, unknown K" --> DB["ğŸŒ€ DBSCAN"]
    F -- "Hierarchical view" --> HC["ğŸŒ² Hierarchical Clustering"]

    style Q   fill:#10B981,stroke:none,color:#000
    style LR  fill:#022c22,stroke:#10B981,color:#6EE7B7
    style MLR fill:#022c22,stroke:#10B981,color:#6EE7B7
    style NB  fill:#022c22,stroke:#10B981,color:#6EE7B7
    style DT  fill:#022c22,stroke:#10B981,color:#6EE7B7
    style RF  fill:#065f46,stroke:none,color:#a7f3d0
    style XGB fill:#059669,stroke:none,color:#fff
    style KM  fill:#022c22,stroke:#6EE7B7,color:#6EE7B7
    style DB  fill:#022c22,stroke:#6EE7B7,color:#6EE7B7
    style HC  fill:#022c22,stroke:#6EE7B7,color:#6EE7B7
    style ASS fill:#065f46,stroke:none,color:#a7f3d0
    style PCA_N fill:#065f46,stroke:none,color:#a7f3d0
    style B   fill:#064e3b,stroke:#10B981,color:#6EE7B7
    style C   fill:#064e3b,stroke:#10B981,color:#6EE7B7
    style D   fill:#064e3b,stroke:#10B981,color:#6EE7B7
    style E   fill:#064e3b,stroke:#10B981,color:#6EE7B7
    style F   fill:#064e3b,stroke:#10B981,color:#6EE7B7
```

---

## ğŸ“ Regression vs Classification

<div align="center">

| Feature | ğŸ“ˆ Regression | ğŸ·ï¸ Classification |
|---------|:----------:|:-------------:|
| **Output** | Continuous number | Discrete class label |
| **Example** | House price, temperature | Spam/Not spam, Disease |
| **Loss Function** | MSE Â· MAE Â· RMSE | Cross-entropy Â· Hinge |
| **Evaluation** | RÂ², RMSE, MAE | Accuracy, F1, AUC-ROC |
| **Algorithms** | Linear, Ridge, Lasso, SVR | Logistic, SVM, NB, KNN, Trees |
| **Output Layer** | Linear activation | Sigmoid / Softmax |

</div>

---

## ğŸŒ² Ensemble Methods Guide

```mermaid
flowchart LR
    subgraph BAGGING ["ğŸ’ BAGGING â€” Parallel"]
        direction TB
        D1["Bootstrap\nSample 1"] --> T1["Tree 1"]
        D2["Bootstrap\nSample 2"] --> T2["Tree 2"]
        D3["Bootstrap\nSample N"] --> TN["Tree N"]
        T1 & T2 & TN --> AGG["Aggregate\nVote / Avg"]
        AGG --> BAG_OUT["ğŸŒ² Random Forest\nPrediction"]
    end

    subgraph BOOSTING ["âš¡ BOOSTING â€” Sequential"]
        direction TB
        W1["Weighted\nData"] --> M1["Weak\nModel 1"]
        M1 -->|"Re-weight\nerrors"| M2["Weak\nModel 2"]
        M2 -->|"Re-weight\nerrors"| M3["Weak\nModel 3"]
        M3 --> BOOST_OUT["ğŸš€ AdaBoost /\nXGBoost Prediction"]
    end

    style BAGGING  fill:#022c22,stroke:#10B981,color:#6EE7B7
    style BOOSTING fill:#064e3b,stroke:#6EE7B7,color:#a7f3d0
    style BAG_OUT  fill:#10B981,stroke:none,color:#000
    style BOOST_OUT fill:#059669,stroke:none,color:#fff
```

| Method | Parallel? | Reduces | Best For | Sklearn API |
|--------|:---------:|---------|---------|-------------|
| **Bagging** | âœ… Yes | Variance | High-variance models | `BaggingClassifier` |
| **Random Forest** | âœ… Yes | Variance + correlation | General purpose â­ | `RandomForestClassifier` |
| **AdaBoost** | âŒ Sequential | Bias | Weak learner stacking | `AdaBoostClassifier` |
| **Gradient Boosting** | âŒ Sequential | Bias | Structured data | `GradientBoostingClassifier` |
| **XGBoost** | âŒ Sequential | Bias + overfitting | Competitions â­ | `xgb.XGBClassifier` |

---

## ğŸ”µ Clustering Algorithms

```mermaid
flowchart LR
    subgraph KM_BOX ["ğŸ”µ K-Means"]
        direction TB
        KM1["1. Place K\nrandom centroids"]
        KM2["2. Assign each point\nto nearest centroid"]
        KM3["3. Recompute\ncentroid positions"]
        KM4["4. Repeat until\nconverged"]
        KM1 --> KM2 --> KM3 --> KM4 --> KM2
    end

    subgraph DB_BOX ["ğŸŒ€ DBSCAN"]
        direction TB
        DB1["For each point:\nFind Îµ-neighbourhood"]
        DB2["Core Point:\nâ‰¥ MinPts neighbours"]
        DB3["Border Point:\nin core's neighbour"]
        DB4["Noise Point:\nnot reachable"]
        DB1 --> DB2 & DB3 & DB4
    end

    subgraph HC_BOX ["ğŸŒ² Hierarchical"]
        direction TB
        HC1["Start: each point\nis its own cluster"]
        HC2["Merge two\nclosest clusters"]
        HC3["Repeat until\none cluster"]
        HC4["Cut dendrogram\nat desired height"]
        HC1 --> HC2 --> HC3 --> HC4
    end

    style KM_BOX fill:#022c22,stroke:#10B981,color:#6EE7B7
    style DB_BOX fill:#064e3b,stroke:#6EE7B7,color:#a7f3d0
    style HC_BOX fill:#047857,stroke:none,color:#a7f3d0
```

| Algorithm | Needs K? | Shape | Handles Noise | Scalable | Best For |
|-----------|:--------:|-------|:------------:|:--------:|---------|
| **K-Means** | âœ… Yes | Spherical | âŒ No | âœ… Yes | Well-separated blobs |
| **DBSCAN** | âŒ No | Arbitrary | âœ… Yes | âš ï¸ Medium | Irregular clusters, anomalies |
| **Hierarchical** | âŒ No | Arbitrary | âŒ No | âŒ Small data | When hierarchy matters |
| **Gaussian Mixture** | âœ… Yes | Elliptical | âš ï¸ Soft | âœ… Yes | Overlapping distributions |

---

## ğŸ“ Metrics Cheatsheet

### ğŸ“ˆ Regression Metrics

<div align="center">

| Metric | Formula | Range | Ideal | Sensitive to Outliers |
|--------|---------|-------|-------|-----------------------|
| **MAE** | `Î£|yáµ¢ âˆ’ Å·áµ¢| / n` | 0 â†’ âˆ | **0** | âŒ Robust |
| **MSE** | `Î£(yáµ¢ âˆ’ Å·áµ¢)Â² / n` | 0 â†’ âˆ | **0** | âœ… Very Sensitive |
| **RMSE** | `âˆšMSE` | 0 â†’ âˆ | **0** | âœ… Sensitive |
| **RÂ² Score** | `1 âˆ’ SS_res/SS_tot` | âˆ’âˆ â†’ 1 | **1.0** | âš ï¸ Moderate |
| **Adjusted RÂ²** | Penalises extra features | âˆ’âˆ â†’ 1 | **1.0** | âš ï¸ Moderate |

</div>

### ğŸ·ï¸ Classification Metrics

<div align="center">

| Metric | Formula | When to Use |
|--------|---------|------------|
| **Accuracy** | `(TP+TN) / Total` | Balanced classes |
| **Precision** | `TP / (TP+FP)` | Cost of false positives is high (spam filter) |
| **Recall** | `TP / (TP+FN)` | Cost of false negatives is high (cancer detection) |
| **F1-Score** | `2 Ã— PrecisionÃ—Recall / (P+R)` | Imbalanced classes â­ |
| **AUC-ROC** | Area under TPR vs FPR curve | Ranking models / thresholds |
| **Log Loss** | `âˆ’Î£ylog(Å·)+(1âˆ’y)log(1âˆ’Å·)` | Probability calibration |

</div>

### ğŸ”µ Clustering Metrics

<div align="center">

| Metric | Range | Ideal | Description |
|--------|-------|-------|-------------|
| **Inertia (WCSS)** | 0 â†’ âˆ | Low | Within-cluster sum of squares |
| **Silhouette Score** | âˆ’1 â†’ +1 | **+1** | Cohesion vs separation |
| **Davies-Bouldin** | 0 â†’ âˆ | **0** | Avg similarity ratio of clusters |
| **Calinski-Harabasz** | 0 â†’ âˆ | High | Ratio of between/within cluster variance |

</div>

---

## âš™ï¸ Feature Engineering Pipeline

```mermaid
flowchart LR
    RAW(["ğŸ“¥ Raw Dataset"]) --> MISS

    subgraph CLEAN ["ğŸ§¹ Step 1 â€” Clean"]
        MISS["Handle\nMissing Values\nDrop / Fill / Impute"]
        DUP["Remove\nDuplicates"]
        MISS --> DUP
    end

    DUP --> FEAT

    subgraph FEAT ["ğŸ”§ Step 2 â€” Feature Work"]
        ENC["Encode\nCategoricals\nOHE / Label"]
        OUT["Remove\nOutliers\nIQR / Z-score"]
        SEL["Feature\nSelection\nCorr / Elim"]
        ENC --> OUT --> SEL
    end

    SEL --> SCALE

    subgraph SCALE ["âš–ï¸ Step 3 â€” Scale"]
        STD["Standardize\nStandardScaler\nÎ¼=0, Ïƒ=1"]
        NRM["Normalize\nMinMaxScaler\n[0, 1]"]
    end

    SCALE --> SPLIT

    subgraph SPLIT ["âœ‚ï¸ Step 4 â€” Split"]
        TRN["Train 80%"]
        VAL["Validation 10%"]
        TST["Test 10%"]
    end

    SPLIT --> MODEL(["ğŸ¤– Model Training"])

    style RAW   fill:#10B981,stroke:none,color:#000
    style MODEL fill:#059669,stroke:none,color:#fff
    style CLEAN fill:#022c22,stroke:#10B981,color:#6EE7B7
    style FEAT  fill:#064e3b,stroke:#6EE7B7,color:#a7f3d0
    style SCALE fill:#022c22,stroke:#10B981,color:#6EE7B7
    style SPLIT fill:#047857,stroke:none,color:#a7f3d0
```

---

## ğŸ’» Quick Code Reference

### âš¡ Complete ML Pipeline (Scikit-learn)

```python
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import xgboost as xgb

# â”€â”€ 1. Load & split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df = pd.read_csv('data.csv')
X = df.drop('target', axis=1)
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# â”€â”€ 2. Scale â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test  = scaler.transform(X_test)

# â”€â”€ 3. Train â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model = RandomForestClassifier(n_estimators=200, random_state=42)
model.fit(X_train, y_train)

# â”€â”€ 4. Evaluate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
print("AUC-ROC:", roc_auc_score(y_test, model.predict_proba(X_test)[:,1]))

# â”€â”€ 5. Tune with GridSearchCV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
params = {'n_estimators': [100, 200], 'max_depth': [None, 10, 20]}
grid   = GridSearchCV(RandomForestClassifier(), params, cv=5, scoring='f1')
grid.fit(X_train, y_train)
print("Best params:", grid.best_params_)
```

### ğŸ”µ K-Means Clustering with Elbow Method

```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

inertia = []
K_range = range(1, 11)

for k in K_range:
    km = KMeans(n_clusters=k, random_state=42, n_init=10)
    km.fit(X_scaled)
    inertia.append(km.inertia_)

# Plot Elbow
plt.figure(figsize=(8, 4))
plt.plot(K_range, inertia, 'go-', linewidth=2, markersize=8)
plt.xlabel('Number of Clusters K')
plt.ylabel('Inertia (WCSS)')
plt.title('Elbow Method â€” Optimal K')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Final model with optimal K
optimal_k = 4
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
labels = kmeans.fit_predict(X_scaled)
```

### ğŸ“‰ PCA + Visualization

```python
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Fit PCA
pca = PCA(n_components=0.95)           # Keep 95% variance
X_pca = pca.fit_transform(X_scaled)

print(f"Original features : {X_scaled.shape[1]}")
print(f"PCA components    : {X_pca.shape[1]}")
print(f"Variance explained: {pca.explained_variance_ratio_.cumsum()[-1]:.2%}")

# Scree plot
plt.bar(range(1, len(pca.explained_variance_ratio_)+1),
        pca.explained_variance_ratio_, color='#10B981')
plt.xlabel('Principal Component')
plt.ylabel('Variance Explained')
plt.title('PCA Scree Plot')
plt.tight_layout()
plt.show()
```

### ğŸ›’ Association Rules (Apriori)

```python
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

# Encode transactions
te = TransactionEncoder()
te_array = te.fit_transform(transactions)
df_enc = pd.DataFrame(te_array, columns=te.columns_)

# Mine frequent itemsets
frequent_items = apriori(df_enc, min_support=0.05, use_colnames=True)

# Generate rules
rules = association_rules(frequent_items, metric='lift', min_threshold=1.5)
rules.sort_values('lift', ascending=False).head(10)
```

---

## ğŸ“ Folder Structure

```
ğŸ“‚  Machine Learning Course/
â”‚
â”œâ”€â”€ ğŸ““ 01_intro_to_ml.ipynb
â”‚   â””â”€â”€ â†’ ML types Â· Workflow Â· Supervised vs Unsupervised
â”‚
â”œâ”€â”€ ğŸ““ 02_feature_engineering.ipynb
â”‚   â””â”€â”€ â†’ Cleaning Â· Encoding Â· Scaling Â· Feature selection
â”‚
â”œâ”€â”€ ğŸ““ 03_linear_regression.ipynb
â”‚   â””â”€â”€ â†’ Simple Â· Multiple Â· OLS Â· RÂ² Â· RMSE
â”‚
â”œâ”€â”€ ğŸ““ 03b_regularization.ipynb
â”‚   â””â”€â”€ â†’ Ridge (L2) Â· Lasso (L1) Â· ElasticNet
â”‚
â”œâ”€â”€ ğŸ““ 03c_project_heating_cooling.ipynb
â”‚   â””â”€â”€ ğŸ—ï¸ Project: Heating & Cooling Load Prediction
â”‚
â”œâ”€â”€ ğŸ““ 04_logistic_regression.ipynb
â”‚   â””â”€â”€ â†’ Sigmoid Â· Confusion matrix Â· Imbalanced datasets
â”‚
â”œâ”€â”€ ğŸ““ 04b_project_diabetes.ipynb
â”‚   â””â”€â”€ ğŸ—ï¸ Project: Diabetic Patient Classification
â”‚
â”œâ”€â”€ ğŸ““ 05_naive_bayes.ipynb
â”‚   â””â”€â”€ â†’ Bayes theorem Â· Gaussian Â· Multinomial Â· Bernoulli
â”‚
â”œâ”€â”€ ğŸ““ 05b_project_news_classification.ipynb
â”‚   â””â”€â”€ ğŸ—ï¸ Project: News Article Classification
â”‚
â”œâ”€â”€ ğŸ““ 06_knn.ipynb
â”‚   â””â”€â”€ â†’ Distance metrics Â· Choosing K Â· Classification + Regression
â”‚
â”œâ”€â”€ ğŸ““ 06b_project_social_ads.ipynb
â”‚   â””â”€â”€ ğŸ—ï¸ Project: Social Network Ads Classification
â”‚
â”œâ”€â”€ ğŸ““ 07_decision_tree.ipynb
â”‚   â””â”€â”€ â†’ Gini Â· Entropy Â· Pruning Â· Visualisation
â”‚
â”œâ”€â”€ ğŸ““ 07b_random_forest.ipynb
â”‚   â””â”€â”€ â†’ Bagging Â· Feature importance Â· OOB score
â”‚
â”œâ”€â”€ ğŸ““ 07c_project_iris.ipynb
â”‚   â””â”€â”€ ğŸ—ï¸ Project: Iris Flower Classification
â”‚
â”œâ”€â”€ ğŸ““ 08_svm.ipynb
â”‚   â””â”€â”€ â†’ Hyperplane Â· Kernels (RBF/Poly) Â· C parameter Â· SVR
â”‚
â”œâ”€â”€ ğŸ““ 09_ensemble_adaboost_xgboost.ipynb
â”‚   â””â”€â”€ â†’ Bagging Â· Boosting Â· AdaBoost Â· XGBoost Â· GBDT
â”‚
â”œâ”€â”€ ğŸ““ 10_gradient_descent.ipynb
â”‚   â””â”€â”€ â†’ Batch Â· Stochastic Â· Mini-batch GD
â”‚
â”œâ”€â”€ ğŸ““ 10b_cross_validation.ipynb
â”‚   â””â”€â”€ â†’ K-Fold Â· Stratified Â· LOOCV
â”‚
â”œâ”€â”€ ğŸ““ 10c_hyperparameter_tuning.ipynb
â”‚   â””â”€â”€ â†’ GridSearchCV Â· RandomizedSearchCV Â· Pipelines
â”‚
â”œâ”€â”€ ğŸ““ 11_kmeans_clustering.ipynb
â”‚   â””â”€â”€ â†’ Elbow method Â· Silhouette Â· Shopping dataset
â”‚
â”œâ”€â”€ ğŸ““ 11b_project_shopping_clustering.ipynb
â”‚   â””â”€â”€ ğŸ—ï¸ Project: Customer Segmentation
â”‚
â”œâ”€â”€ ğŸ““ 12_hierarchical_clustering.ipynb
â”‚   â””â”€â”€ â†’ Dendrogram Â· Single/Complete linkage Â· Ward
â”‚
â”œâ”€â”€ ğŸ““ 12b_dbscan.ipynb
â”‚   â””â”€â”€ â†’ Epsilon Â· MinPts Â· Core/Border/Noise points
â”‚
â”œâ”€â”€ ğŸ““ 13_association_rules.ipynb
â”‚   â””â”€â”€ â†’ Apriori Â· FP-Growth Â· Support Â· Confidence Â· Lift
â”‚
â”œâ”€â”€ ğŸ““ 13b_project_market_basket.ipynb
â”‚   â””â”€â”€ ğŸ—ï¸ Project: Market Basket Analysis
â”‚
â”œâ”€â”€ ğŸ““ 14_pca.ipynb
â”‚   â””â”€â”€ â†’ Eigenvectors Â· Scree plot Â· Variance explained
â”‚
â””â”€â”€ ğŸ“„ README.md
```

---

## ğŸ† Projects Showcase

```mermaid
journey
    title ML Project Journey â€” Difficulty Progression
    section Regression
      Heating and Cooling Load Prediction: 7: Me
    section Classification
      Diabetic Patient Classification: 7: Me
      News Article Classification NLP: 7: Me
      Social Network Ads Classification: 8: Me
      Iris Flower Multi-class: 8: Me
    section Ensemble
      Customer Churn XGBoost: 8: Me
    section Clustering
      Customer Segmentation K-Means: 8: Me
      Shopping Dataset DBSCAN: 7: Me
    section Association
      Market Basket Apriori: 7: Me
    section Advanced
      End-to-End Pipeline Capstone: 6: Me
```

<div align="center">

| # | ğŸ—ï¸ Project | ğŸ¤– Algorithm | ğŸ“Š Dataset | ğŸ¯ Task | Best Score |
|---|-----------|------------|----------|--------|-----------|
| 01 | **Heating & Cooling Load Prediction** | Linear Regression Â· Ridge | ENB2012 | Regression | RMSE = 1.42 |
| 02 | **Diabetic Patient Classification** | Logistic Regression | Pima Indians | Binary Clf | Acc = 79% |
| 03 | **News Article Classification** | NaÃ¯ve Bayes | 20 Newsgroups | Multi-class | Acc = 91% |
| 04 | **Social Network Ads** | KNN | Social Network | Binary Clf | Acc = 93% |
| 05 | **Iris Flower Classification** | Decision Tree | Iris Dataset | Multi-class | Acc = 97% |
| 06 | **Customer Churn Modelling** | Random Forest Â· XGBoost | Telecom | Binary Clf | F1 = 0.88 |
| 07 | **Customer Segmentation** | K-Means (k=5) | Mall Shopping | Clustering | Sil = 0.55 |
| 08 | **Shopping â€” Advanced** | DBSCAN Â· Hierarchical | Mall Shopping | Clustering | â€” |
| 09 | **Market Basket Analysis** | Apriori Â· FP-Growth | Grocery Store | Assoc Rules | Lift = 3.2 |

</div>

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone & Navigate

```bash
git clone https://github.com/MuhammadZafran33/Data-Science-Course.git
cd "Data-Science-Course/Data Science Full Course By WsCube Tech/ Machine Learning Course"
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install scikit-learn pandas numpy matplotlib seaborn xgboost mlxtend \
            statsmodels scipy imbalanced-learn jupyter
```

### 3ï¸âƒ£ Launch Notebooks

```bash
jupyter notebook
```

<div align="center">

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MuhammadZafran33/Data-Science-Course/)

</div>

---

## ğŸ”— Navigation

<div align="center">

| â¬…ï¸ Previous Module | ğŸ“ You Are Here | â¡ï¸ Next Module |
|-------------------|----------------|---------------|
| [ğŸ” EDA Projects](../EDA/) | **ğŸ¤– Machine Learning Course** | [ğŸ§  Deep Learning â†’](../DEEP%20LEARNING/) |

</div>

---

<div align="center">

<br/>

[![GitHub](https://img.shields.io/badge/GitHub-MuhammadZafran33-181717?style=for-the-badge&logo=github&logoColor=white)](https://github.com/MuhammadZafran33)

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%"/>

> *"In God we trust. All others must bring data."*
> **â€” W. Edwards Deming**

<br/>

**â­ Found this useful? Star the repo and keep learning! â­**

<img src="https://readme-typing-svg.demolab.com?font=Fira+Code&size=14&pause=1000&color=10B981&center=true&vCenter=true&width=700&lines=Every+Algorithm+You+Learn+Unlocks+a+New+Superpower+%F0%9F%9A%80;Keep+Building%2C+Keep+Iterating%2C+Keep+Learning+%F0%9F%A4%96" alt="Footer"/>

</div>
