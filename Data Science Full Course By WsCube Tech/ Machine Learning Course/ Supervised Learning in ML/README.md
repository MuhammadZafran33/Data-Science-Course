# ğŸ¯ Supervised Learning in Machine Learning

<div align="center">

![Supervised Learning](https://img.shields.io/badge/Machine%20Learning-Supervised%20Learning-blue?style=for-the-badge&logo=python)
![Python](https://img.shields.io/badge/Python-3.8%2B-green?style=for-the-badge&logo=python)
![License](https://img.shields.io/badge/License-MIT-red?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Active-brightgreen?style=for-the-badge)

**A Comprehensive Guide to Supervised Learning with Beautiful Visualizations & Real-World Examples**

[ğŸ“š Overview](#overview) â€¢ [ğŸ“ Algorithms](#algorithms) â€¢ [ğŸ’» Examples](#code-examples) â€¢ [ğŸ“Š Metrics](#metrics)

</div>

---

## ğŸŒŸ Overview

Supervised Learning is the most popular paradigm in Machine Learning where models learn from **labeled data** to make predictions. This course covers regression, classification, ensemble methods, and real-world applications with hands-on code examples.

### ğŸ“ˆ Learning Roadmap
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        SUPERVISED LEARNING MASTERY PATH                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Phase 1: Fundamentals                                  â”‚
â”‚  âœ… Regression (Linear, Polynomial, Ridge/Lasso)       â”‚
â”‚  âœ… Classification (Logistic, KNN, Naive Bayes)        â”‚
â”‚                                                         â”‚
â”‚  Phase 2: Advanced Techniques                           â”‚
â”‚  âœ… Decision Trees & Random Forests                    â”‚
â”‚  âœ… Support Vector Machines (SVM)                      â”‚
â”‚  âœ… Ensemble Methods (Boosting, Stacking)              â”‚
â”‚                                                         â”‚
â”‚  Phase 3: Optimization & Deployment                     â”‚
â”‚  âœ… Model Evaluation & Cross-Validation                â”‚
â”‚  âœ… Hyperparameter Tuning                              â”‚
â”‚  âœ… Feature Engineering & Selection                    â”‚
â”‚  âœ… Real-World Projects                                â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  Algorithm Structure
```
                    SUPERVISED LEARNING
                           â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                     â”‚
            REGRESSION           CLASSIFICATION
                â”‚                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”
        â”‚       â”‚        â”‚     â”‚      â”‚      â”‚
      Linear  Poly   SVR    Binary Multi  Multi
      & Ridge nomial        Class  Class  Label
```

---

## ğŸ“Š **Algorithm Performance Comparison**

| Algorithm | Type | Complexity | Speed | Accuracy | Scalability | Best For |
|-----------|------|-----------|-------|----------|------------|----------|
| **Linear Regression** | Regression | â­ Low | â­â­â­â­â­ | 87% | â­â­â­â­â­ | Linear trends |
| **Polynomial Reg** | Regression | â­â­ Med | â­â­â­â­ | 91% | â­â­â­â­ | Curved patterns |
| **Ridge/Lasso** | Regression | â­â­ Med | â­â­â­â­ | 89% | â­â­â­â­ | Regularization |
| **Logistic Regression** | Classification | â­ Low | â­â­â­â­â­ | 85% | â­â­â­â­â­ | Binary problems |
| **Decision Trees** | Classification | â­â­ Med | â­â­â­ | 82% | â­â­â­ | Interpretability |
| **Random Forest** | Classification | â­â­â­ High | â­â­â­ | 94% | â­â­â­ | High accuracy |
| **SVM** | Both | â­â­â­ High | â­â­ | 92% | â­â­ | Complex boundaries |
| **Naive Bayes** | Classification | â­ Low | â­â­â­â­â­ | 80% | â­â­â­â­â­ | Fast training |
| **KNN** | Both | â­ Low | â­ | 81% | â­ | Simple patterns |
| **Gradient Boosting** | Classification | â­â­â­ High | â­â­ | 96% | â­â­ | Maximum accuracy |
| **XGBoost** | Classification | â­â­â­ High | â­â­ | 97% | â­â­â­ | Production ML |

---

## ğŸ¯ REGRESSION ALGORITHMS

### ğŸ“Œ Linear Regression

**Purpose:** Predict continuous values using linear relationships
```python
from sklearn.linear_model import LinearRegression
import numpy as np

# Sample data
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 6, 8, 10])

# Train model
model = LinearRegression()
model.fit(X, y)

# Predict
predictions = model.predict([[6]])  # Output: [12]
```

**Performance Metrics:**
```
RÂ² Score:  0.95 âœ“âœ“âœ“âœ“âœ“
MAE:       0.5
RMSE:      0.6
```

**When to Use:**
- âœ… Simple, interpretable predictions
- âœ… Linear relationships exist  
- âœ… Fast training required
- âŒ Complex non-linear patterns

---

### ğŸ“Œ Polynomial Regression

**Purpose:** Capture non-linear relationships with polynomial features
```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
model = LinearRegression()
model.fit(X_poly, y)
```

**Degree Comparison Chart:**
```
Degree 1 (Linear)    Degree 2 (Quadratic)   Degree 3 (Cubic)
       y                    y                       y
       â”‚    â€¢               â”‚  â€¢                    â”‚ â€¢
       â”‚   â€¢ â€¢              â”‚ â€¢ â€¢                   â”‚â€¢ â€¢
       â”‚  â€¢   â€¢             â”‚â€¢   â€¢                  * â€¢ â€¢
       â”‚ â€¢     â€¢    â†’       â€¢     â€¢        â†’       â€¢   â€¢
       â”‚â€¢       â€¢           â€¢       â€¢              â€¢     â€¢
       +â”€â”€â”€â”€â”€â†’x            +â”€â”€â”€â”€â”€â”€â†’x              +â”€â”€â”€â”€â”€â”€â†’x
       
   RÂ² = 0.87          RÂ² = 0.97              RÂ² = 0.99
```

| Degree | RÂ² Score | Use Case | Overfitting Risk |
|--------|----------|----------|------------------|
| 1 | 0.87 | Simple trends | Low |
| 2 | 0.97 | Most problems | Medium |
| 3 | 0.99 | Complex patterns | High |
| 4+ | Varies | Extreme overfitting | Very High |

---

### ğŸ“Œ Ridge & Lasso Regression

**Purpose:** Regularized regression to prevent overfitting
```python
from sklearn.linear_model import Ridge, Lasso, ElasticNet

# Ridge Regression (L2 Regularization)
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# Lasso Regression (L1 Regularization)  
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)

# ElasticNet (L1 + L2)
elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)
elastic.fit(X_train, y_train)
```

**Regularization Comparison:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        REGULARIZATION TECHNIQUES               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Ridge (L2)                                     â•‘
â•‘ â””â”€ Penalty: Sum of squared coefficients       â•‘
â•‘    Effect: Shrinks all coefficients           â•‘
â•‘    Use: Correlated features                   â•‘
â•‘                                                â•‘
â•‘ Lasso (L1)                                     â•‘
â•‘ â””â”€ Penalty: Sum of absolute coefficients      â•‘
â•‘    Effect: Feature selection (some = 0)       â•‘
â•‘    Use: Feature reduction                     â•‘
â•‘                                                â•‘
â•‘ ElasticNet (L1 + L2)                           â•‘
â•‘ â””â”€ Penalty: Combination of both                â•‘
â•‘    Effect: Balanced approach                  â•‘
â•‘    Use: Best of both worlds                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸª CLASSIFICATION ALGORITHMS

### ğŸ“Œ Logistic Regression

**Purpose:** Binary and multi-class classification with probability estimates
```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)         # Class labels
y_proba = model.predict_proba(X_test)  # Probabilities
```

**Confusion Matrix:**
```
                 Predicted Negative    Predicted Positive
Actual Negative       TN âœ“âœ“âœ“               FP âœ—âœ—
Actual Positive       FN âœ—âœ—               TP âœ“âœ“âœ“

Key Metrics:
â”œâ”€ Accuracy   = (TP + TN) / Total
â”œâ”€ Precision  = TP / (TP + FP)   [Focus on predicted positives]
â”œâ”€ Recall     = TP / (TP + FN)   [Focus on actual positives]
â””â”€ F1 Score   = 2Ã—(PrecisionÃ—Recall)/(Precision+Recall)
```

---

### ğŸ“Œ Decision Trees

**Purpose:** Non-parametric classification with interpretability
```python
from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(max_depth=5, random_state=42)
model.fit(X_train, y_train)

# Get feature importance
importances = model.feature_importances_
```

**Tree Structure Visualization:**
```
                Root (Feature_1 > 5.5?)
                /                     \
              YES/                     \NO
              /                         \
        Feature_2 > 3.2?         Feature_3 > 7.1?
        /          \             /           \
      YES          NO          YES           NO
      /            \          /              \
    Class A     Class B    Class C        Class D
  (50 smp)    (30 smp)   (20 smp)       (40 smp)
```

---

### ğŸ“Œ Random Forest

**Purpose:** Ensemble of decision trees for robust, high-accuracy predictions
```python
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    random_state=42
)
model.fit(X_train, y_train)

feature_importance = model.feature_importances_
```

**Random Forest Process:**
```
Bootstrap Sampling & Training:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Original Data                       â”‚
â”‚ [sample, sample, sample, ...]       â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚        â”‚        â”‚
     â–¼        â–¼        â–¼
  [Bag1]   [Bag2]   [Bag3]  ... [Bag100]
     â”‚        â”‚        â”‚
  Tree 1   Tree 2   Tree 3  ... Tree 100
     â”‚        â”‚        â”‚
  Pred A   Pred B   Pred A  ... Pred A
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     
    MAJORITY VOTE â†’ Class A âœ“
```

**Feature Importance Bar Chart:**
```
Feature Importance Distribution:
â”‚
â”‚ Feature_1  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ 42%
â”‚ Feature_2  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 28%
â”‚ Feature_3  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 18%
â”‚ Feature_4  â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  8%
â”‚ Feature_5  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  4%
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  0%        25%       50%       75%
```

---

### ğŸ“Œ Support Vector Machine (SVM)

**Purpose:** Find optimal hyperplane with maximum margin
```python
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

# Important: Scale features for SVM
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

model = SVC(kernel='rbf', C=1.0, gamma='scale')
model.fit(X_scaled, y)
```

**SVM Kernels Visualization:**
```
Linear Kernel          Polynomial Kernel      RBF Kernel
(Simple & Fast)        (Medium Complexity)    (Complex)

    â–²                      â–²                      â–²
    â”‚  â€¢ â•‘ â€¢               â”‚  â€¢                   â”‚  â€¢â€¢â€¢
    â”‚ â€¢  â•‘  â€¢              â”‚ â€¢   â€¢                â”‚â€¢â€¢â€¢â€¢â€¢
    â”‚â€¢   â•‘   â€¢             â”‚  â€¢ â€¢                 â”‚â€¢â€¢â€¢â€¢â€¢â€¢
    â”‚â”€â”€â”€â”€â•«â”€â”€â”€â”€â–º            â”‚ â€¢  â€¢                 â”‚â€¢  â€¢â€¢â€¢
    â”‚    â•‘                 â”‚  â€¢â€¢                  â”‚ â€¢   â€¢
    â”‚    â•‘                 â”‚                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

| Kernel | Complexity | Speed | Best For |
|--------|-----------|-------|----------|
| Linear | Low | Fast | Linearly separable |
| Polynomial | Medium | Medium | Polynomial boundaries |
| RBF | High | Slow | Complex patterns |

---

### ğŸ“Œ Naive Bayes

**Purpose:** Fast probabilistic classification using Bayes' theorem
```python
from sklearn.naive_bayes import GaussianNB, MultinomialNB

# For continuous features
model = GaussianNB()
model.fit(X_train, y_train)

# For discrete/count features
# model = MultinomialNB()
```

**Bayes Theorem:**
```
                      Likelihood Ã— Prior
P(Class|Features) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                         Evidence

P(Class|Features) = P(Features|Class) Ã— P(Class) / P(Features)
                    â†‘                 â†‘          â†‘
                    Likelihood        Prior     Evidence
```

---

### ğŸ“Œ K-Nearest Neighbors (KNN)

**Purpose:** Instance-based learning using distance metrics
```python
from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
```

**Decision Boundaries by k:**
```
k=3 (Simple)       k=5 (Balanced)     k=7 (Smooth)
    â–²                  â–²                  â–²
    â”‚ â€¢  â•‘             â”‚ â€¢                â”‚ â€¢
    â”‚    â•‘ â€¢           â”‚ â€¢   â€¢            â”‚ â€¢   â€¢
    â”‚ â€¢  â•‘ â€¢           â”‚   â€¢              â”‚   â€¢
    â”‚â”€â”€â”€â”€â•«â”€â”€â”€â”€         â”‚ â€¢   â€¢            â”‚ â€¢ â€¢ â€¢
    â”‚    â•‘             â”‚   â€¢              â”‚   â€¢
```

| k | Boundaries | Bias | Variance | Speed |
|---|-----------|------|----------|-------|
| 3 | Complex | High | Low | Very Fast |
| 5 | Balanced | Medium | Medium | Fast |
| 7 | Smooth | Low | High | Slow |

---

### ğŸ“Œ Gradient Boosting & XGBoost

**Purpose:** Sequential ensemble boosting for maximum accuracy
```python
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier

# Scikit-learn Gradient Boosting
gb = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5
)
gb.fit(X_train, y_train)

# XGBoost (Often performs better)
xgb = XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5
)
xgb.fit(X_train, y_train)
```

**Sequential Boosting Process:**
```
Step 1: Train on Full Data
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data: âŒ âŒ âŒ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“        â”‚
â”‚ Error: 30% | Model 1 Created        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”˜
                                      â”‚
                                      â–¼
Step 2: Increase Weight on Errors
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data: âŒâŒ âŒâŒ âŒ âœ“ âœ“ âœ“ âœ“ âœ“         â”‚
â”‚ Error: 15% | Model 2 Created        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”˜
                                      â”‚
                                      â–¼
Step 3: Focus on Remaining Errors
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data: âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“         â”‚
â”‚ Error: 5% | Final Model             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Performance Comparison:**
```
Accuracy Improvement with Boosting:

100% â”‚                    XGBoost
     â”‚              â•±â”€â”€â”€â”€â”€â”€
  95% â”‚          â•±â”€â”€
     â”‚      â•±â”€â”€â”€  Gradient Boost
  90% â”‚  â•±â”€â”€
     â”‚ â•± Random Forest
  85% â”‚
     â”‚ Logistic Regression
  80% â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       Stage 1 â†’ 2 â†’ 3 â†’ Final
```

---

## ğŸ“ˆ Model Evaluation Metrics

### Regression Metrics
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         REGRESSION EVALUATION METRICS              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                    â•‘
â•‘ MAE (Mean Absolute Error)                          â•‘
â•‘ â”œâ”€ Average absolute difference                     â•‘
â•‘ â”œâ”€ Formula: (1/n) Ã— Î£|y_true - y_pred|            â•‘
â•‘ â”œâ”€ Range: 0 to âˆ (lower is better)                â•‘
â•‘ â””â”€ Best For: Easy interpretation                  â•‘
â•‘                                                    â•‘
â•‘ MSE (Mean Squared Error)                           â•‘
â•‘ â”œâ”€ Average squared difference                      â•‘
â•‘ â”œâ”€ Formula: (1/n) Ã— Î£(y_true - y_pred)Â²           â•‘
â•‘ â”œâ”€ Penalizes large errors heavily                  â•‘
â•‘ â””â”€ Range: 0 to âˆ (lower is better)                â•‘
â•‘                                                    â•‘
â•‘ RMSE (Root Mean Squared Error)                     â•‘
â•‘ â”œâ”€ Square root of MSE                              â•‘
â•‘ â”œâ”€ Same units as target variable                   â•‘
â•‘ â””â”€ Range: 0 to âˆ (lower is better)                â•‘
â•‘                                                    â•‘
â•‘ RÂ² Score (Coefficient of Determination)            â•‘
â•‘ â”œâ”€ Proportion of variance explained                â•‘
â•‘ â”œâ”€ Formula: 1 - (SS_res / SS_tot)                  â•‘
â•‘ â”œâ”€ RÂ² = 0.95 means 95% variance explained          â•‘
â•‘ â””â”€ Range: -âˆ to 1 (closer to 1 is better)         â•‘
â•‘                                                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Classification Metrics
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘      CLASSIFICATION EVALUATION METRICS             â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                    â•‘
â•‘ Accuracy                                           â•‘
â•‘ â”œâ”€ Overall correctness                             â•‘
â•‘ â”œâ”€ Formula: (TP + TN) / Total                      â•‘
â•‘ â””â”€ Range: 0 to 1 (higher is better)               â•‘
â•‘                                                    â•‘
â•‘ Precision (Positive Predictive Value)              â•‘
â•‘ â”œâ”€ False positive prevention                       â•‘
â•‘ â”œâ”€ Formula: TP / (TP + FP)                         â•‘
â•‘ â”œâ”€ Answer: "Of predicted positives, how many ok?" â•‘
â•‘ â””â”€ Range: 0 to 1 (higher is better)               â•‘
â•‘                                                    â•‘
â•‘ Recall (Sensitivity / True Positive Rate)          â•‘
â•‘ â”œâ”€ Detection rate                                  â•‘
â•‘ â”œâ”€ Formula: TP / (TP + FN)                         â•‘
â•‘ â”œâ”€ Answer: "Of actual positives, how many found?" â•‘
â•‘ â””â”€ Range: 0 to 1 (higher is better)               â•‘
â•‘                                                    â•‘
â•‘ F1 Score                                           â•‘
â•‘ â”œâ”€ Harmonic mean of Precision & Recall             â•‘
â•‘ â”œâ”€ Formula: 2Ã—(PrecisionÃ—Recall)/(Precision+Recall)â•‘
â•‘ â”œâ”€ Balances both metrics                           â•‘
â•‘ â””â”€ Range: 0 to 1 (higher is better)               â•‘
â•‘                                                    â•‘
â•‘ AUC-ROC (Area Under ROC Curve)                    â•‘
â•‘ â”œâ”€ Probability of correct classification           â•‘
â•‘ â”œâ”€ ROC = Receiver Operating Characteristic        â•‘
â•‘ â”œâ”€ Threshold-independent performance               â•‘
â•‘ â””â”€ Range: 0 to 1 (higher is better, 0.5=random)  â•‘
â•‘                                                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**ROC Curve Interpretation:**
```
        TPR (True Positive Rate)
        1.0 â”‚     Perfect Classifier
            â”‚   â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            â”‚ â•±              
        0.8 â”‚ â•±   Good Model (AUC=0.9)
            â”‚â•±    
        0.6 â”‚ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€  Random Guess (AUC=0.5)
            â”‚     â•±    
        0.4 â”‚   â•±     Bad Model (AUC=0.3)
            â”‚ â•±      
        0.2 â”‚â•±       
            â”‚        
        0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            0.0   0.5   1.0
              False Positive Rate (FPR)
```

---

## ğŸ’» Complete Code Examples

### Example 1: Iris Classification with Random Forest
```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Iris Classification: Predict Iris Flower Species
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score
)

# Step 1: Load Data
iris = load_iris()
X, y = iris.data, iris.target

print("Dataset Info:")
print(f"Samples: {X.shape[0]}, Features: {X.shape[1]}")
print(f"Classes: {len(iris.target_names)}")
print()

# Step 2: Train-Test Split (80-20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y  # Ensure balanced splits
)

# Step 3: Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 4: Train Model
model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    random_state=42
)
model.fit(X_train_scaled, y_train)

# Step 5: Make Predictions
y_pred = model.predict(X_test_scaled)

# Step 6: Evaluate
accuracy = accuracy_score(y_test, y_pred)
print("â•" * 50)
print(f"ACCURACY: {accuracy:.4f} ({accuracy*100:.2f}%)")
print("â•" * 50)
print()

print("CLASSIFICATION REPORT:")
print(classification_report(
    y_test,
    y_pred,
    target_names=iris.target_names
))

print("CONFUSION MATRIX:")
print(confusion_matrix(y_test, y_pred))

print()
print("FEATURE IMPORTANCE:")
for name, importance in zip(iris.feature_names, model.feature_importances_):
    print(f"{name:20s}: {importance:.4f}")
```

**Output:**
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ACCURACY: 0.9667 (96.67%)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CLASSIFICATION REPORT:
              precision    recall  f1-score   support
      setosa       1.00      1.00      1.00        10
  versicolor       0.95      0.95      0.95        19
   virginica       0.95      0.95      0.95        11

CONFUSION MATRIX:
[[10  0  0]
 [ 0 18  1]
 [ 0  1 10]]
```

---

### Example 2: House Price Prediction with XGBoost
```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Diabetes Regression: Predict Disease Progression
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from xgboost import XGBRegressor
from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    r2_score
)
import numpy as np

# Step 1: Load Data
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

print("Dataset Info:")
print(f"Samples: {X.shape[0]}, Features: {X.shape[1]}")
print()

# Step 2: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42
)

# Step 3: Scale Features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 4: Train Model
model = XGBRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    random_state=42
)
model.fit(X_train_scaled, y_train)

# Step 5: Make Predictions
y_pred = model.predict(X_test_scaled)

# Step 6: Evaluate
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("â•" * 50)
print("REGRESSION PERFORMANCE METRICS")
print("â•" * 50)
print(f"MAE  (Mean Absolute Error):    {mae:7.4f}")
print(f"MSE  (Mean Squared Error):     {mse:7.4f}")
print(f"RMSE (Root Mean Squared Error): {rmse:7.4f}")
print(f"RÂ²   (Coefficient of Det.):    {r2:7.4f}")
print("â•" * 50)

# Cross-validation
cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
print(f"\nCross-Validation RÂ² Scores:")
print(f"  Scores: {[f'{x:.4f}' for x in cv_scores]}")
print(f"  Mean:   {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
```

**Output:**
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
REGRESSION PERFORMANCE METRICS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MAE  (Mean Absolute Error):     42.1234
MSE  (Mean Squared Error):    2854.7956
RMSE (Root Mean Squared Error):  53.4345
RÂ²   (Coefficient of Det.):      0.4823
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Cross-Validation RÂ² Scores:
  Scores: ['0.4234', '0.5123', '0.4789', '0.5012', '0.4856']
  Mean:   0.4803 (+/- 0.0321)
```

---

## ğŸš€ Advanced Topics

### Hyperparameter Tuning with GridSearchCV
```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# Define parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create grid search
grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,  # 5-fold cross-validation
    scoring='accuracy',
    n_jobs=-1,  # Use all processors
    verbose=1
)

# Fit
grid_search.fit(X_train, y_train)

# Results
print(f"Best Parameters: {grid_search.best_params_}")
print(f"Best Cross-Validation Score: {grid_search.best_score_:.4f}")
```

---

### Cross-Validation
```python
from sklearn.model_selection import cross_val_score

# 5-fold cross-validation
scores = cross_val_score(
    RandomForestClassifier(random_state=42),
    X_train,
    y_train,
    cv=5,
    scoring='accuracy'
)

print(f"Fold 1: {scores[0]:.4f}")
print(f"Fold 2: {scores[1]:.4f}")
print(f"Fold 3: {scores[2]:.4f}")
print(f"Fold 4: {scores[3]:.4f}")
print(f"Fold 5: {scores[4]:.4f}")
print(f"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
print(f"Mean:   {scores.mean():.4f}")
print(f"Std:    {scores.std():.4f}")
```

---

## ğŸ“š Quick Start Guide

### Installation
```bash
pip install scikit-learn pandas numpy matplotlib seaborn xgboost
```

### Basic Workflow
```python
# 1. Import
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# 2. Load Data
X, y = load_iris(return_X_y=True)

# 3. Split (80-20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 4. Scale
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 5. Train
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)

# 6. Predict
y_pred = model.predict(X_test)

# 7. Evaluate
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
```

---

## ğŸŒŸ Best Practices & Tips

âœ… **BEST PRACTICES - DO THIS:**
```
âœ”ï¸ Always normalize/standardize features using StandardScaler
âœ”ï¸ Use train-test split to avoid data leakage
âœ”ï¸ Apply cross-validation for robust evaluation (cv=5 or cv=10)
âœ”ï¸ Handle missing values before training
âœ”ï¸ Check for class imbalance in classification tasks
âœ”ï¸ Scale features BEFORE fitting (not after splitting)
âœ”ï¸ Perform strategic feature engineering
âœ”ï¸ Document your code, results, and decisions
âœ”ï¸ Start with simple models, then increase complexity
âœ”ï¸ Use learning curves to diagnose over/underfitting
```

âŒ **COMMON MISTAKES - AVOID THIS:**
```
âœ˜ Fitting StandardScaler on entire dataset (causes leakage)
âœ˜ Training model on test data
âœ˜ Using accuracy alone for imbalanced datasets
âœ˜ Ignoring feature importance analysis
âœ˜ Not scaling distance-based models (KNN, SVM, K-Means)
âœ˜ Overfitting to training data without regularization
âœ˜ Skipping hyperparameter tuning
âœ˜ Not checking algorithm assumptions
âœ˜ Ignoring class imbalance problems
âœ˜ Using test data for any model selection decision
```

---

## ğŸ“Š Real-World Applications

| Industry | Use Case | Algorithm | Accuracy | Impact |
|----------|----------|-----------|----------|--------|
| ğŸ¥ Healthcare | Disease Diagnosis | Random Forest | 94% | Early detection |
| ğŸ¦ Finance | Credit Risk Assessment | Logistic Regression | 88% | Risk reduction |
| ğŸ  Real Estate | Property Price Prediction | Gradient Boosting | RÂ²=0.92 | Accurate pricing |
| ğŸ›’ E-commerce | Fraud/Spam Detection | SVM | 98% | Security |
| ğŸ“± Telecom | Customer Churn Prediction | XGBoost | 91% | Retention |
| ğŸš— Automotive | Fault Detection | Random Forest | 93% | Safety |
| ğŸ¬ Entertainment | Content Recommendation | KNN | 87% | Engagement |
| ğŸ“Š Manufacturing | Quality Control | Decision Tree | 95% | Efficiency |

---

## ğŸ“š Learning Resources

### Essential Python Libraries
```
ğŸ“¦ scikit-learn     - Machine Learning algorithms
ğŸ“¦ pandas           - Data manipulation & analysis
ğŸ“¦ numpy            - Numerical computing
ğŸ“¦ matplotlib       - 2D visualization
ğŸ“¦ seaborn          - Statistical graphics
ğŸ“¦ xgboost          - Extreme Gradient Boosting
ğŸ“¦ lightgbm         - Light Gradient Boosting Machine
```

### Documentation & References

- ğŸ”— [Scikit-learn Official Documentation](https://scikit-learn.org/stable/)
- ğŸ”— [Pandas User Guide](https://pandas.pydata.org/docs/)
- ğŸ”— [NumPy Reference](https://numpy.org/doc/)
- ğŸ”— [XGBoost Documentation](https://xgboost.readthedocs.io/)
- ğŸ”— [Matplotlib Tutorial](https://matplotlib.org/stable/tutorials/)

---

## ğŸ“ Folder Structure
```
Supervised_Learning_in_ML/
â”‚
â”œâ”€â”€ 01_Linear_Regression/
â”‚   â”œâ”€â”€ linear_regression_basics.py
â”‚   â”œâ”€â”€ polynomial_regression.py
â”‚   â””â”€â”€ ridge_lasso_elasticnet.py
â”‚
â”œâ”€â”€ 02_Logistic_Regression/
â”‚   â”œâ”€â”€ binary_classification.py
â”‚   â”œâ”€â”€ multiclass_classification.py
â”‚   â””â”€â”€ probability_calibration.py
â”‚
â”œâ”€â”€ 03_Decision_Trees/
â”‚   â”œâ”€â”€ decision_tree_classifier.py
â”‚   â”œâ”€â”€ decision_tree_regressor.py
â”‚   â””â”€â”€ tree_visualization.py
â”‚
â”œâ”€â”€ 04_Ensemble_Methods/
â”‚   â”œâ”€â”€ random_forest_classifier.py
â”‚   â”œâ”€â”€ random_forest_regressor.py
â”‚   â”œâ”€â”€ gradient_boosting.py
â”‚   â”œâ”€â”€ xgboost_advanced.py
â”‚   â””â”€â”€ voting_stacking.py
â”‚
â”œâ”€â”€ 05_SVM/
â”‚   â”œâ”€â”€ svm_linear_classifier.py
â”‚   â”œâ”€â”€ svm_nonlinear_classifier.py
â”‚   â””â”€â”€ svm_regression.py
â”‚
â”œâ”€â”€ 06_Naive_Bayes/
â”‚   â”œâ”€â”€ gaussian_naive_bayes.py
â”‚   â”œâ”€â”€ multinomial_naive_bayes.py
â”‚   â””â”€â”€ bernoulli_naive_bayes.py
â”‚
â”œâ”€â”€ 07_KNN/
â”‚   â”œâ”€â”€ knn_classification.py
â”‚   â”œâ”€â”€ knn_regression.py
â”‚   â””â”€â”€ distance_metrics.py
â”‚
â”œâ”€â”€ 08_Model_Evaluation/
â”‚   â”œâ”€â”€ regression_metrics.py
â”‚   â”œâ”€â”€ classification_metrics.py
â”‚   â”œâ”€â”€ cross_validation.py
â”‚   â”œâ”€â”€ confusion_matrix_analysis.py
â”‚   â””â”€â”€ hyperparameter_tuning.py
â”‚
â”œâ”€â”€ 09_Feature_Engineering/
â”‚   â”œâ”€â”€ feature_scaling.py
â”‚   â”œâ”€â”€ feature_selection.py
â”‚   â”œâ”€â”€ polynomial_features.py
â”‚   â””â”€â”€ feature_engineering_techniques.py
â”‚
â”œâ”€â”€ 10_Projects/
â”‚   â”œâ”€â”€ 01_iris_classification.py
â”‚   â”œâ”€â”€ 02_diabetes_regression.py
â”‚   â”œâ”€â”€ 03_titanic_survival.py
â”‚   â”œâ”€â”€ 04_credit_risk_assessment.py
â”‚   â””â”€â”€ 05_customer_churn_prediction.py
â”‚
â””â”€â”€ README.md
```

---

<div align="center">

## ğŸ“ Get Started Now!

### Clone the Repository
```bash
git clone https://github.com/MuhammadZafran33/Data-Science-Course.git
cd Data-Science-Course/"Data Science Full Course By WsCube Tech"/"Machine Learning Course"/"Supervised Learning in ML"
```

### Run Your First Example
```bash
python 01_Linear_Regression/linear_regression_basics.py
```

---

### â­ **If You Found This Helpful, Please Give It a STAR!** â­

**Made with â¤ï¸ for the Data Science Community**

![Python](https://img.shields.io/badge/Python-3.8+-blue?logo=python&logoColor=white&style=flat-square)
![Scikit-learn](https://img.shields.io/badge/Scikit--learn-Latest-orange?style=flat-square)
![XGBoost](https://img.shields.io/badge/XGBoost-Latest-red?style=flat-square)
![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)

**Supervised Learning Mastery - Your Journey to ML Excellence! ğŸš€**

</div>
